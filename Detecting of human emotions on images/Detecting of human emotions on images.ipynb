{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build a Little VGG for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing the libraries\n",
    "import numpy as np\n",
    "import keras\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Flatten, Activation\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 6\n",
    "img_rows = 48\n",
    "img_cols = 48\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path of the images\n",
    "train_data_dir = './train/'\n",
    "validation_data_dir = './validation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28273 images belonging to 6 classes.\n",
      "Found 3534 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "#Lets use the data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=30,\n",
    "      shear_range=0.3,\n",
    "      zoom_range=0.3,\n",
    "      width_shift_range=0.4,\n",
    "      height_shift_range=0.4,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_set = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    " \n",
    "validation_set = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Little VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 256)         295168    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 390       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 1,328,102\n",
      "Trainable params: 1,325,926\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#Block 1 CONV ==> RELU ==> CONV ==> RELU ==> POOLING\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', kernel_initializer='he_normal', input_shape=(img_rows, img_cols, 1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, kernel_size=(3,3) , padding='same', kernel_initializer='he_normal', input_shape=(img_rows, img_cols, 1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Block 2 CONV ==> RELU ==> CONV ==> RELU ==> POOLING\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Blcok 3 CONV ==> RELU ==> CONV ==> RELU ==> POOLING\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Blcok 4 CONV ==> RELU ==> CONV ==> RELU ==> POOLING\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#BLOCK 5: first set of FC ==> RELU layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=64, kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#BLOCK 6: second set of FC ==> RELU layers\n",
    "model.add(Dense(units=64, kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#Block 7: softmax classifier\n",
    "model.add(Dense(units = num_classes, kernel_initializer='he_normal' ))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#print the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "checkpoint = ModelCheckpoint(r'D:\\\\Backup_of_exact_MY_REPO\\\\Detection of human emotions on images\\\\human_emotion.h5',\n",
    "                            monitor = 'val_loss',\n",
    "                            mode='min',\n",
    "                            save_best_only=True,\n",
    "                            verbose=1)\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=1, restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1, min_delta=0.0001)\n",
    "\n",
    "callbacks = [checkpoint, early_stop, reduce_lr]\n",
    "\n",
    "#compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_on_training_set = 28273\n",
    "images_on_validation_set = 3534\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_set, \n",
    "                              steps_per_epoch = images_on_training_set//batch_size , \n",
    "                              epochs= epochs, \n",
    "                              verbose=1,\n",
    "                             callbacks=callbacks, \n",
    "                              validation_data = validation_set,\n",
    "                             validation_steps = images_on_validation_set//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran the model with the same parameters in the another machine which having higher computation speed than this and and i got the model saved. Now i am going to get the model from there rather than running the above one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the model\n",
    "from keras.models import load_model\n",
    "classifier = load_model(r'C:\\\\Users\\\\Dell\\\\Desktop\\\\KPML_practice\\\\Master Computer Vision\\\\DeepLearningCV\\\\Trained Models\\\\emotion_little_vgg_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 28273\n",
    "num_val = 3534"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3534 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "#here we need to create the validation generator with shuffle = false to get the prediction correct for each class\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "                                                            validation_data_dir,\n",
    "                                                            color_mode = 'grayscale',\n",
    "                                                            target_size=(img_rows, img_cols),\n",
    "                                                            batch_size=batch_size,\n",
    "                                                            class_mode='categorical',\n",
    "                                                            shuffle= False)\n",
    "\n",
    "#predicting the validation set\n",
    "y_pred = classifier.predict_generator(validation_generator, num_val//batch_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54059464, 0.24996917, 0.06406295, 0.01067813, 0.07242453,\n",
       "       0.06227062], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the y_pred is in probabilities and we need to convert it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "cm = confusion_matrix(y_pred, validation_generator.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[182,  89,  24,  83,  39,  11],\n",
       "       [ 42,  37,  12,  32,  29,  23],\n",
       "       [ 33,  39, 749, 154,  54,  24],\n",
       "       [100, 130,  54, 173, 222,  20],\n",
       "       [ 99,  97,  11,  90, 229,   9],\n",
       "       [ 35, 136,  29,  94,  21, 329]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get our class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Angry': 0, 'Fear': 1, 'Happy': 2, 'Neutral': 3, 'Sad': 4, 'Surprise': 5}\n"
     ]
    }
   ],
   "source": [
    "class_indices = validation_set.class_indices\n",
    "print(class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('Angry', 0), ('Fear', 1), ('Happy', 2), ('Neutral', 3), ('Sad', 4), ('Surprise', 5)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_indices.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Angry': 0, 'Fear': 1, 'Happy': 2, 'Neutral': 3, 'Sad': 4, 'Surprise': 5}\n"
     ]
    }
   ],
   "source": [
    "classes_labels = {v:k for k,v in class_indices.items()}\n",
    "print(class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Angry', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n"
     ]
    }
   ],
   "source": [
    "classes = list(classes_labels.values())\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.YTick at 0x2b8446a1390>,\n",
       "  <matplotlib.axis.YTick at 0x2b844692c88>,\n",
       "  <matplotlib.axis.YTick at 0x2b827bc06d8>,\n",
       "  <matplotlib.axis.YTick at 0x2b84471a748>,\n",
       "  <matplotlib.axis.YTick at 0x2b84471ac18>,\n",
       "  <matplotlib.axis.YTick at 0x2b844721160>],\n",
       " <a list of 6 Text yticklabel objects>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAFmCAYAAACC84ZkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XucVXW9//HXWwRRSVBRI7ToQlqZopKJt7yWmoWWpulJNAtPadntlMc6RfnrYlYoliYnU7C8pZlWJhJpqR1UMPKSqaSSBImIlxREmPn8/vh+Z9hOc92z9mWN7+fjsR6z93evvfZnhs36rO91KSIwMzMDWK/RAZiZWfNwUjAzs3ZOCmZm1s5JwczM2jkpmJlZOycFMzNr56RgZmbtnBTMzKydk4KZmbVbv9EBmJkNFO/ad+N4ckVL1e+ff/fqWRFxUIEh9ZmTgplZQZ5c0cIds15d9fsHjXpoZIHhVMVJwcysIAG00troMPrFfQpmZtbONQUzs8IELVHumoKTgplZQVLzUblvR+CkYGZWoLL3KTgpmJkVJAhaSn7jMnc0m5lZO9cUzMwK5D4FMzMDUkdzi5OCmZm1cU3BzMyAXFNwR7OZmQ0UrimYmRWo3LMUnBTMzAoThDuazcwsC2gpd05wn4KZma3jpGBmVpC0IF71W08kbStpQcX2rKRPSdpM0mxJD+Wfm+b9JWmapIWS7pa0c0+f4aRgZlYY0dKPrScR8UBEjIuIccAuwErgGuA0YE5EjAXm5OcABwNj8zYZOL+nz3BSMDMrSACtUf3WR/sDf4uIRcBEYEYunwEclh9PBGZGMhcYIWlUdwd1R7OZWYF6c8VfkKOBy/LjrSJiKUBELJW0ZS4fDTxW8Z7FuWxpVwd1TcHMrHmMlDSvYpvc2U6ShgDvBX7Ww/E6y1Dd1klcUzAzK0haEK9fNYXlETG+F/sdDNwVEY/n549LGpVrCaOAZbl8MbBNxfu2BpZ0d2DXFMzMCtQaqnrrgw+yrukI4DpgUn48Cbi2ovy4PAppN+CZtmamrrimYGZWkAJqCj2StBFwIHBSRfG3gCslnQj8HTgyl18PHAIsJI1UOqGn4zspmJkVJBAtNW6AiYiVwOYdyp4kjUbquG8AJ/fl+G4+MjOzdq4pmJkVqI99A03HScHMrCD16FOotQGfFAYP2TiGbrRpo8Pok/VeWNvoEPokXlzT6BD6TEMGNzqEvltbsu9FS7nuLPACz/NirO7nGV20RLlb5Qd8Uhi60abstNcnGx1Gn2z04JONDqFPWv/+j0aH0GeDRnc7078pxfIVjQ6hT1qefbbRIfTJ7TGn0SE0hQGfFMzM6iWtkuqagpmZZe5TMDMzACLcp2BmZhVaS15TKHdKMzOzQrmmYGZWkDRPodzX2k4KZmaFcZ+CmZllHpJqZmYv0VLytY/KndLMzKxQrimYmRWkHvdTqDUnBTOzArW6o9nMzGBgDEktd/RmZlYo1xTMzAoSqPSjj5wUzMwKVPZ5CjWJXtLhkkLSdrU4vplZM4qAlliv6q0Z1CqKDwK3AkcXcTBJrtGYWQmI1n5szaDwpCBpGLAHcCI5KUjaR9LNkq6S9FdJP5Wk/NohuexWSdMk/SqXT5E0XdKNwExJt0gaV/E5t0naoej4zcxezmpxBX4YcENEPChphaSdc/lOwFuAJcBtwB6S5gEXAHtHxCOSLutwrF2APSNilaRJwPHApyS9EdggIu6uQfxmZlUJaJpmoGrVIvoPApfnx5fn5wB3RMTiiGgFFgBjgO2AhyPikbxPx6RwXUSsyo9/BhwqaTDwYeDirgKQNFnSPEnz1rz4fH9/HzOzXmthvaq3ZlBoTUHS5sB+wPaSAhhESp7XA6srdm3Jn91TI1r7GT0iVkqaDUwEPgCM7+pNETEdmA7wihFbR99/EzOzvgtEq4ekvsQRwMyIOKmtQNLvgT272P+vwOskjYmIR4Gjejj+j4BfArdExIoC4jUzK1SzXPFXq+joPwhc06HsauCYznbOTUMfB26QdCvwOPBMVwePiPnAs8BFhURrZmYvUWhNISL26aRsGjCtQ9kpFU9viojt8mikHwDz8j5TOh5L0qtIiezG4qI2MytGUP4F8Zoh+o9KWgDcBwwnjUb6N5KOA24Hvpg7q83Mmoxo6cfWDBo+KSwipgJTe7HfTGBm7SMyM6uOawpmZjagNLymYGY2kDRLM1C1nBTMzAoSITcfmZnZOrVeJVXSiIp15O6XNEHSZpJmS3oo/9w076u8ptxCSXdXLDvUJScFM7OCBNRjldRzSOvLbQfsCNwPnAbMiYixwJz8HOBgYGzeJgPn93RwJwUzs5KQtAmwN3AhQES8GBFPk5b/mZF3m0FamJRcPjOSucAISaO6+wz3KZiZFUa1XiX1dcATwEWSdgTmA6cCW0XEUoCIWCppy7z/aOCxivcvzmVLu/oA1xTMzAqS5imo6g0Y2bbCc94md/iI9YGdgfMjYifSoqGn0bXO2qS6XSTUNQUzswL1c0G85RHR5QrQpCv9xRFxe35+FSkpPC5pVK4ljAKWVey/TcX7tybd06ZLrimYmRWkbensftQUuj9+xD+BxyRtm4v2B/4CXAdMymWTgGvz4+uA4/IopN2AZ9qambrimoKZWbl8AvippCHAw8AJpAv8KyWdCPwdODLvez1wCLAQWJn37ZaTgplZgVpr3AATEQvo/CZj+3eybwAn9+X4TgpmZgWJgBbfec3MzNqU/Xac7mg2M7N2A76msN5zq9notgcbHUafREu57iEUL77Y6BD6rPWfy3reqdm0lut7ocFDGh1C36zp/xV+Gn1U7mvtAZ8UzMzqyUtnm5kZsG5Gc5k5KZiZFab8zUfljt7MzArlmoKZWYH6cF+EpuSkYGZWEE9eMzOzl3CfgpmZDRiuKZiZFaRt6ewyc1IwMyuQO5rNzAzw5DUzM+vAHc1mZjZguKZgZlaUXt5ruZk5KZiZFSRwR7OZmVVwTcHMzICBMfrIHc1mZtaurjUFSS3APRVFh0XEo/WMwcyslspeU6h389GqiBhX1MEkCVBElOvmtWY2IA2EZS4a3nwkaZCksyTdKeluSSfl8mGS5ki6S9I9kibm8jGS7pd0HnAXsE0j4zczq9SKqt6aQb1rChtKWpAfPxIRhwMnAs9ExNskbQDcJulG4DHg8Ih4VtJIYK6k6/J7twVOiIiP1zl+M7MBrRmaj94J7CDpiPx8ODAWWAx8Q9LeQCswGtgq77MoIuZ29SGSJgOTAYaut3GB4ZuZdSPcp1AEAZ+IiFkvKZSOB7YAdomINZIeBYbml5/v7oARMR2YDjB8/S2i6IDNzDrjIanFmAV8TNJgAElvlLQxqcawLCeEfYHXNDJIM7PeaM1LXVSzNYNmqCn8CBgD3JVHEz0BHAb8FPilpHnAAuCvDYvQzKwXBsLoo7omhYgY1klZK3B63jqa0MWhti8yLjMzS5qhpmBmNmCEawpmZtamWeYbVMtJwcysIOEhqWZmVqnszUfNMCTVzMx6SdKjeemfBXl0JpI2kzRb0kP556a5XJKmSVqYlxHauafjOymYmRWm+jkKfWx22jcixkXE+Pz8NGBORIwF5uTnAAeTVogYS1rl4fyeDuykYGZWoAhVvfXDRGBGfjyDNNerrXxmJHOBEZJGdXcgJwUzs4K0LXNR45pCADdKmp/XeQPYKiKWAuSfW+by0aTFRdsszmVdckezmVnzGNnWT5BNz2u5VdojIpZI2hKYLam71R46yzTdrgfnpGBmVpRIw1L7YXlFP0HnHxGxJP9cJukaYFfgcUmjImJpbh5alndfzEvvObM1sKS747v5yMysQLW8yY6kjSW9ou0x6dYD9wLXAZPybpOAa/Pj64Dj8iik3Uj3rlna3We4pmBmVpCg5vMUtgKuSWuHsj5waUTcIOlO4EpJJwJ/B47M+18PHAIsBFYCJ/T0AU4KZmaFqe0qqRHxMLBjJ+VPAvt3Uh7AyX35DDcfmZlZO9cUzMwK1M+O5oZzUjAzK1DZ1z4a+EkhglizttFR9IkGDWp0CH0y6x9/anQIfXbAsR9udAh9NuTex3reqYnEs882OoS6i3BSMDOzCmVfOtsdzWZm1s41BTOzArmj2czM2rlPwczMAAj6vQR2w7lPwczM2rmmYGZWoJJ3KTgpmJkVxvMUzMzsJUpeVXBSMDMrUNlrCu5oNjOzdq4pmJkVyJPXzMwMqMud12rOScHMrCgBOCmYmVmbsjcfuaPZzMzauaZgZlakktcUnBTMzArjBfHaSXquw/PjJX2/qOObmZVC9GNrAu5TMDOzdnVJCpLeI+l2SX+S9FtJW+XyKZIukfQ7SQ9J+mgu30fSHyRdI+kvkn4oaT1JJ0qaWnHcj0r6Xj1+BzOzHuUF8ardmkGRfQobSlpQ8Xwz4Lr8+FZgt4gISR8BPg98Nr+2A7AbsDHwJ0m/zuW7Am8GFgE3AO8DLgfulvT5iFgDnACcVODvYGbWP03SDFStIpPCqogY1/ZE0vHA+Px0a+AKSaOAIcAjFe+7NiJWAask3URKBk8Dd0TEw/lYlwF7RsRVkn4HHCrpfmBwRNzTMRBJk4HJAEO1cYG/oplZT5rjir9a9epTOBf4fkS8lXRlP7TitY55NXoo/xFwPKmWcFFnHxYR0yNifESMH6Khne1iZlYb7mjuleHAP/LjSR1emyhpqKTNgX2AO3P5rpJeK2k94ChSExQRcTuwDXAMcFmtAzczezmpV1KYAvxM0i3A8g6v3QH8GpgLnBERS3L5/wHfAu4lNTddU/GeK4HbIuKpWgZtZtZnJa8pFNanEBHDOjy/GLg4P74WuLaLtz4YEZM7KV8ZEUd18Z49galdvGZm1hgDYEG8Us1TkDRC0oOkTu05jY7HzKyjiOq3ZtDQZS4iYkoX5TcDN3dS/jTwxpoGZWbWH01ycq9WqWoKZmZWW14Qz8ysSCXvU3BSMDMrkNx8ZGZmQP+Go/YhmUgalNeS+1V+/tq8vtxDkq6QNCSXb5CfL8yvj+np2E4KZmblcypwf8XzM4GpETEWeAo4MZefCDwVEW8gDeM/s6cDOymYmRVGqU+h2q03nyBtDbybtOQPkgTsB1yVd5kBHJYfT8zPya/vn/fvkpOCmVmR+td8NFLSvIqts4m9Z5NWmm7NzzcHno6Itfn5YmB0fjwaeAwgv/5M3r9L7mg2MytS/zqal0fE+K5elHQosCwi5kvap624myi6e61TTgpmZkWq7eijPYD3SjqEtNr0JqSawwhJ6+fawNZA2xpyi0kLiC6WtD5pcdIV3X2Am4/MzEoiIv47IraOiDHA0cDvIuJY4CbgiLzbJNatNXcd61amPiLv323aclIwMytK24J4Nexo7sIXgM9IWkjqM7gwl18IbJ7LPwOc1tOB3HxkZlagek1eq1wjLt+lctdO9nkBOLIvx3VSMDMrkmc0m5nZQOGkYGZm7QZ+89GQweg1o3ver4m0DNug0SH0ySE7HtjoEPqsZZdBjQ6hz5468PWNDqFPhl85r9Eh9FEx7T5lXxBv4CcFM7N68tLZZmYG9Hm102bkPgUzM2vnmoKZWZFKXlNwUjAzK5A7ms3MbB0nBTMza1fypOCOZjMza+eagplZQRTuUzAzs0qevGZmZu1KXlNwn4KZmbVzTcHMrEDuUzAzs3WcFMzMDACPPjIzs5coeVJwR7OZmbVzTcHMrEgvx5qCpJD03Yrnn5M0pcpjjZD08Srf+6ikkdW818ysFtpmNVezNYNqm49WA+8r6IQ8Aug0KUgq3410zcxKrNqksBaYDny64wuStpB0taQ787ZHLp8i6XMV+90raQzwLeD1khZIOkvSPpJuknQpcE/e9xeS5ku6T9LkKmM2M6u96MfWBPrTp/AD4G5J3+5Qfg4wNSJulfRqYBbwpm6OcxqwfUSMA5C0D7BrLnsk7/PhiFghaUPgTklXR8ST/YjdzMw6UXVSiIhnJc0EPgmsqnjpAODNUvuiUJtIekUfD39HRUIA+KSkw/PjbYCxQJdJIdcmJgMMHbxJHz/azKxKTdQ3UK3+jj46G7gLuKiibD1gQkRUJgokreWlzVVDuznu8xXv24eUaCZExEpJN/fwXiJiOql5i+Ebjir5P5GZlUrJzzj9mqcQESuAK4ETK4pvBE5peyJpXH74KLBzLtsZeG0u/xfQXU1iOPBUTgjbAbv1J2Yzs5oqeZ9CEZPXvgtUjkL6JDBe0t2S/gL8Zy6/GthM0gLgY8CDALlv4Lbc8XxWJ8e/AVhf0t3AGcDcAmI2M7NOVNV8FBHDKh4/DmxU8Xw5cFQn71kFvLOL4x3ToejmitdWAwd38b4xfQjbzKymhPsUzMyskpOCmZkBHn1kZmYdlDwpeJVUMzNr56RgZlakGg5JlTRU0h2S/pyX/flqLn+tpNslPSTpCklDcvkG+fnC/PqYnj7DScHMrEA1XiV1NbBfROwIjAMOkrQbcCZpeaGxwFOsmzt2Imme1xuAqXm/bjkpmJkVqYY1hUiey08H5y2A/YCrcvkM4LD8eGJ+Tn59f1WsQdQZJwUzs6L0JyGkpDBS0ryK7d9WhZY0KE8CXgbMBv4GPB0Ra/Mui4HR+fFo4DGA/PozwObd/QoefWRm1jyWR8T47naIiBZgnKQRwDV0vgp1W72js1pBt3US1xTMzApUrzuvRcTTpNUfdgNGSGq7yN8aWJIfLyatLE1+fTiworvjOimYmRWptqOPtsg1BPL9ZQ4A7gduAo7Iu00Crs2Pr8vPya//LiK6/SQ3H5mZFajGM5pHATPyrYrXA66MiF/lxUcvl/T/gD8BF+b9LwQukbSQVEM4uqcPcFIwMyuJiLgb2KmT8odJd6zsWP4CcGRfPsNJwcysSCVf5sJJwcysKE10s5xqOSmYmRVEdD4GtEycFMzMiuSaghVt0L9eaHQIfdLyxBONDqHPNnx4k0aH0Gcrt9iy0SH0Saxd2/NOzaTkJ/OiOCmYmRXIN9kxM7N1nBTMzKydk4KZmQED4h7NXvvIzMzauaZgZlakktcUnBTMzApU9uYjJwUzsyKVPCm4T8HMzNq5pmBmViA3H5mZWeJVUs3M7CWcFMzMDPLS2SVPCu5oNjOzdq4pmJkVqeQ1BScFM7MCKcqdFZwUzMyK4tFHZmZWyR3N/SDpi5Luk3S3pAWS3t7L942RdG+t4zMze7lpWE1B0gTgUGDniFgtaSQwpFHxmJkVouQ1hUY2H40ClkfEaoCIWA4g6cvAe4ANgT8CJ0VESNoF+DGwEri1MSGbmXXPzUfVuxHYRtKDks6T9I5c/v2IeFtEbE9KDIfm8ouAT0bEhEYEa2bWK9GPrQk0LClExHPALsBk4AngCknHA/tKul3SPcB+wFskDQdGRMTv89sv6e7YkiZLmidp3ostK2v3S5iZDTANHX0UES3AzcDNOQmcBOwAjI+IxyRNAYaSZo/3Oo9GxHRgOsDwDUc1Sf41swHP92iunqRtJY2tKBoHPJAfL5c0DDgCICKeBp6RtGd+/dj6RWpm1gclbz5qZE1hGHCupBHAWmAhqSnpaeAe4FHgzor9TwB+LGklMKu+oZqZ9WwgLIjXsKQQEfOB3Tt56Ut562z/HSuKptQmMjOzfij5MhdeJdXMzNp5mQszswK5+cjMzJIm6jCulpuPzMwKpNbqtx6PLW0j6SZJ9+d1407N5ZtJmi3pofxz01wuSdMkLcxrzO3c02c4KZiZFam2Q1LXAp+NiDcBuwEnS3ozcBowJyLGAnPyc4CDgbF5mwyc39MHOCmYmZVERCyNiLvy438B9wOjgYnAjLzbDOCw/HgiMDOSucAISaO6+wz3KZiZFaifHc0jJc2reD49r9Dw758jjQF2Am4HtoqIpZASh6Qt826jgccq3rY4ly3tKgAnBTOzogT9naewPCLG97RTXvHhauBTEfGspC537aSs2wDdfGRmViBF9Vuvji8NJiWEn0bEz3Px423NQvnnsly+GNim4u1bA0u6O76TgplZSShVCS4E7o+I71W8dB0wKT+eBFxbUX5cHoW0G/BMWzNTV9x8ZGZWpNrOU9gD+BBwj6QFuex04FvAlZJOBP4OHJlfux44hLS23ErSGnLdclIwMytIrRfEi4hb6byfAGD/TvYP4OS+fIaTgplZUSJKvyCek4KZWYHKvvaRO5rNzKydawpmZkUqeU1h4CeFF9cQi/7R6Cj6pGWHNzQ6hD4ZNHRoo0Pou64n+zStTS+f3+gQ+mTRlzu7h1bzenH63EKOU/bmo4GfFMzM6iWA1nJnBfcpmJlZO9cUzMyKVO6KgpOCmVmR3KdgZmbrePKamZm1KXtNwR3NZmbWzjUFM7Oi9P5ey03LScHMrCBpldRyZwUnBTOzIrU2OoD+cVIwMytQ2WsK7mg2M7N2rimYmRXFHc1mZraO77xmZmYVPHnNzMwGDNcUzMyK5OYjMzMDIEAln6fQq+YjSV+UdJ+kuyUtkPT2WgQj6XpJI2pxbDOzuoiofmsCPdYUJE0ADgV2jojVkkYCQ3pzcEnrR8TaXuyXZ4fHIb05rplZ02qOc3vVelNTGAUsj4jVABGxPCKWSHo0JwgkjZd0c348RdJ0STcCMyUdL+laSTdIekDSV/J+YyTdL+k84C5gm7ZjStpY0q8l/VnSvZKOyu/ZRdLvJc2XNEvSqOL/JGZmL1+9SQo3kk7YD0o6T9I7evGeXYCJEXFMfr4rcCwwDjhS0vhcvi0wMyJ2iohFFe8/CFgSETtGxPbADZIGA+cCR0TELsCPga/3IhYzs7pRRNVbM+gxKUTEc6ST/GTgCeAKScf38LbrImJVxfPZEfFkLvs5sGcuXxQRczt5/z3AAZLOlLRXRDxDSiDbA7MlLQC+BGzd2YdLmixpnqR5L8YLPf2KZmbFGeh9CgAR0QLcDNws6R5gErCWdUllaIe3PN/xEF0877hf2+c9KGkX4BDgm7kp6hrgvoiY0It4pwPTAYYPGtkcf2kzG/iC0q+S2mNNQdK2ksZWFI0DFgGPkmoQAO/v4TAHStpM0obAYcBtPXzmq4CVEfET4DvAzsADwBa54xtJgyW9paf4zczqRVTfdNQszUe9qSkMA87NQ0XXAgtJTUlvAi6UdDpwew/HuBW4BHgDcGlEzJM0ppv93wqcJakVWAN8LCJelHQEME3S8Bz72cB9vfgdzMysF3pMChExH9i9k5duAd7Yyf5TOtl3WUSc0mG/R0l9BJVlY/LDWXnreOwFwN49xWxm1jBNcsVfLc9oNjMrkpNC9yLiYuDiWn+OmVnDvRw6ms3M7OXDScHMrEC1Hn0k6ceSlkm6t6JsM0mzJT2Uf26ayyVpmqSFee26nXs6vpOCmVmRaj957WLSqg+VTgPmRMRYYE5+DnAwMDZvk4Hzezq4k4KZWWH6kRB6mRQi4g/Aig7FE4EZ+fEM0nywtvKZkcwFRvS0ZpyTgplZUYJGLXOxVUQsBcg/t8zlo4HHKvZbnMu65CGpZmbNY6SkeRXPp+dle6qlTsq6zT5OCmZmRerfkNTlETG+593+zeOSRkXE0tw8tCyXLwa2qdhva2BJdwdy85GZWYEatPbRdaSFSsk/r60oPy6PQtoNeKatmakrrimYmRWpxjOaJV0G7ENqaloMfAX4FnClpBOBvwNH5t2vJ602vRBYCZzQ0/GdFMzMihJAa22TQkR8sIuX9u9k3wBO7svx3XxkZmbtXFMwMytM89xBrVpOCmZmRXJSMDOzdiVPCu5TMDOzdgO+pvBs65PLb3xuxqIaHHoksLwGx4U/1uSoUMuYa6N28f61JkcF/43X+eqlNTkstYv5Nf0+Qh1GH9XagE8KEbFFLY4raV6VMw8bpmwxly1eKF/MZYsXmj3mgCj3XXYGfFIwM6urkvcpOCmYmRVlADQfuaO5ev1ZubBRyhZz2eKF8sVctnihnDGXhqLkVR0zs2YxfMhWsftWR1f9/hsWT5vf6P4SNx+ZmRWp5BfaTgpmZoXxMhdmNgBIUkRE289Gx1NaAbSWe0iqO5pfhiR1dou+0pG0aaNj6CtJr8g/m+3f4HWQllpuwtisjpwUCiZplKT1mvU/VuWVoKSPSXpPo2OqhqRtgK9J2rRZ/9aV8p2vXgPMk7RLM518JQ0DLpF0JgyMxNBZ/JLqc76LqH5rAk4KBcmJYCTwc2BCs1bBKxLCocDbgQWNjahqmwJjgGFlOIlFsgi4GLhI0rhmiFvSehHxHPAfwJ6SvgDlTgwVTWEHSvqSpFMljYmo01RjJwUDiIjWiFgOXA58RNLGjY6pK/nG3ucCQyPiMUmDynICkLQFQETcTUpoUyUNadYkDO21hPUAIuKbwCXAZZJ2avTJt+JE+RbgbuBjkk7Pr5UyMeS43w18A7gXeA/wX/X5XSJNXqt2awJOCgWQ9JrcbDQIuBRoAQbl1xr+n6pjDPnG3Z8A9pF0fES0lOEEkJtfvi7potw2fwnpRDYyv9508bddtUZEa1sfSEScBfwvTZIYJB0HfJ1Ui5kCHCxpCpQ3MQB7A4cDrcDGwDfy77JhY8Nqfh591E+S9gLOAm4HNgI+DWwJnAac3ugr2A59CIcDGwJ/iYhfSfoQcFbeZUajY+1MRVPAEGAJ6erv86ST2GbABNINyb/TjPFX/O0/DewoaTDwpYj4nqS1wExJH46IOxsY5kbAtIi4XdKdwH3AxflP/9Vm/Lt2VPE92Tginiclgu8Dw4FjIuIfufYwTNLPataUFFCvVqpacU2hHyRNAD4FfIZ0snoBODP/3E/S6AaGB7zkpHQyKVENB26R9O6ImE2K/WuSjmlgmF3K/9EPAq4AzgDGRsTHSX/v64ClwL6Stm1gmN3Kf/v3Ah8HxgP/K2lCREwDfgp8X9IGdYqls6v+AD6dm+FagT8DdwIHSNq8HnH1R0VCOJj0ewwi1XreAvwyIh6RtDcwFVha874FNx+9POWT0InA7Ij4Y0Q8HhGfAH5A6lcYBryrkTG2kfQ24CjgANK/+SLgXEkfiIjfAccBcxsYYpck7Qp8FvgJsBp4j6TPAssi4kpgEvA4sHXjonypTk68m5P+xicBDwLzgQsk7RUR3wIOiojV9Yit4iLhCEkfkbRjRFwAXE+6WHgDcDSwFjiHpc99AAAMTklEQVQsIp6sR1z90dapDHwXuCkiWkh/448Bx0v6CakP7dMR8Yc6BFTqjmY3H1Uh/6cfCYwCDpT0q4hYnF++PyL+ImkRMEXSZRGxqt7xVVb5I+JOSUeQktT7I2L7fGK9XNKKiPhtPePrLUlbA98DbouIqyXdAOxJSnDbAIsi4m+S1pA6E+d0/N0boeLE+xlSc90ZwFjg0IjYP792OHC0pDsj4qlaxyRpo4hYmR9/Cng/8FvgWElXAF8B/ptUA3slcEozJ4ScvDbN3+3BwEdI/Qa3SXo/8DZSjWd3YCtgvYh4qOaBRXjy2stF29WfpB2AHwLzgP8B/gkckUf0VBpN+jLWVYc+hN0l7ZfbWZcBrwDuybsuAq4GFtY7xj54HrgV+KCkt0fE8xExi1QreB2ApPVJ3+MLYd0JudGU5n/sBFyUY1qRyw+TNJHUPHNmRLxQh1jeDXxD0mhJbycNmd6L9PcdnuP8EPDliPgA8M48uquZbQsMljQsItYANwAnSZoNvANYBRwMEBF/q0tCGCBcU+ilXEXdF3gfsA+pc/kLpDbho4BJkmZGxJKcQJ4H/qPetYQOV6mHAY8AX5X0n8BiUl/HFaST6hGRxs43hYq24Z2BV5ES2JnAY6R+j3OAB0gJ9xmAiFgraXKjk4GkDdqagHJf0juBPYAn8i6rSN+VE0i3fTwmIv5eh7gOJXXKfyV3tq4APpMTxaHArsCXSKPR1pd0PqmZrmnl78mv82iueyV9mPS3fYTUrPgXSbsDBwFDgefqGmBzXJdUzUmhlyTtQuq8+g9S+/vbgWnAKcAQ4APABtB+Yp5T5/heGRH/zI93Bt4REXtL+i/glRFxn6SHgWeBXUgniaZJCNCeePcjDdecC3wNOAf4A6ld/ifA74HjI+KutiTSBAlhY1Lb9Y3Am0hXsT8kNXGdI+mTEfG8pMtIHeZtNbdax/VKUn/MR3Izy4akIZrKcd6Yk+rDwC1A7UblFKhi8MEqUnPXD0jNXXMAJB1A+r/5hUhzh+obX8mbj5wUelDRHDOCNJLhltyGuYD0xfs28F/AHW1ttg2I8d3AV/KIoidIHa/zJf0AeD3pihBg/4j4FWn4bNPJnfcnAcdGxFxJhwHvJg1F/TqpCWZf4F+Ni/Lf5RP+o6RE9iSwXaR5Cf9DGnH0PUmfrag1Pl+n0FYDa4AXJA0l1Wz3JHUibwaMz23zuwPvrUeiKkK+QDsW+N+ImC7pBWB6rjHcArwW+ExE3FD/Pqbm6TCulvsUutDJCJJ/kka+HBwRayLiPuBPpA7nk4HVXQz3q3WcB5GGmn45Ip5QGs//HOlK8E2kJqw1ko4HzpBU936OnigtEbIBqWluB1KbMBHxC9LktNPyrleQRpV8OZ/kmsnDwN9IExffmsvuJ416GUZqBqu3p4FZwHdIfUdjSCPjzgBmk2q5fyB1gD/YgPj6TNImwHnAq9pGEkXETFJH+RXAXsCPIuKG/Fp9z9Btt+Ms8ZBU1xS6kKuo+wPvl/QbUnPQZ4BPKY3dfgjYGfgN6QvaUu8YJW1GGkr4vnxV9Hrgy6ShsleQhmtOkdRKusI+OiIer3ecXam4ihscEatzn8ELwOskTYyIa4HbgAOBITnpTQda69FB21uS3kdKChNIs2gvkXRqRNyUT2IXkNq76yp/hy8A/khqyrq2ot9jMnBXRFxd77j6qqKvSRHxbG4SvVTSpyNiKkBE/CQPOmj46LOyc1LooOILuBupvXIOqQngdcDNpLHQXyS1zZ9K+s+2l6SNgFX1/EJGxIo8yuWM3C48Ffh1RKwFfiFpCanDdivgnIj4W71i60nF3/lg4KOS7iI1v/yANCHwc0rDaEcDU9uaXhrRRtwLO5CS8Ucj4iqlJTgulPRz0sieYxvVNBNpsbv/yxsAko4Etif12TS9/D3ZB3iHpAeA35EmA54vaU1EfD/vdzH8+5Dsumv+bpluOSl0kL+A25Gq3GdEWg5iX1JH8iDSEMN35KuSvUnj6I9sVH9CHoXRQurjOD3S8gmDcxPXHY2IqTuVncOS3gV8k7Q0yHGkq+wzI+Lb+XfaDrgkIn5Z+d6GBd+BpNdExKKImCLpX8APJJ0cERdJWkaaO3FK2wCARlMaNn0U8FHgqGa6SOiOpD2As4ErgeOBHYFfkianXSpp/Yg4u23/Rn5HAogmaQaqlvsUSB2cko5WWqMf0uiMIPUVEBE3kRa6GwdMzrWCIaRO3PdGxD2dHLZucvvpu0gjYIbnPoTBjYypM0ornH5B0ohc9FbSSWoD0pIE04BTlSYfnUOa/ft2pdmqTTMHAdpHeJ0m6b0AEfFd4BrgWkl7RMSvgZMj4v5GxtnB06Rmz4kRcW+jg+mN/H/yy8C3IuIbpH6Qp4EDI+Iu4MOkOUPNISLVFKrdmoCa6P9ZQ+TO4e8AnwR+RhrLP4U0rPBDpBPWKfnK9h3Ak23/oZTWom+Of0kgN8WcTZqctKLR8XSktHjgh0ijic4ijYLZlDTG/NSIuFdp8tFwUj/CMOBI4NJGj4zpWEvJ/UonkYbK/i4nASTdTpqb8P6o09IVA5XSXIM3A0eQOvCPiYhnJL0ZmElKbv9oZIwdbaLNYrf131n1+2evvWJ+RIwvMKQ+e9nXFPJ/9FmkYZr/Q0oGXyOt//Ir0rC+c/K+v6+8wmqmhAAQEb8hrSD6WzXn3d/mkjpdR5D6Y4aQrvqeB57OQw3/QRox9Uz+Dz+tmRKCpEm5k/ZdpOHIy0gL8h2b2+rvIl1EOCH0g9KaV18ljTb7Kmlk1+eV7hLXNrS3Kc9f0RpVb83AfQpARNwo6eOkK5GJkiaREsEBpGGFu0s6N0owVT4irpU0p1kSlqTXAivySX6NpD+TJnatIM02nQL8hbRMxeuBz7YNj8wn46b4PQCUZoUfSxoiewvwFHARMJE0n2Jb4EMR8WijYhwIJL2R1O8xKyL+lPvvhpGajm4DHgVOi4jHGhdlN5rnK1sVNx/lJqB8ZfIe4CpSc8ZU0jDDA0kzP29uXJTlpTS79CrS4mUh6Rekv+tlwAdJE+2+Q2oyekVELGqWDmVJryY1Fz6fm4umkZoZjyR1ih8aad2dtv03a8ZmuzLJfWFvJF0sbEi6SHhA6c51e5JGHa2JiP/O+zfFd6WN0qKNI/txiOURcVBR8VTjZZ8U2kjakpQM9gQ+FWk5YSQNjSYaE19GShPsziN1cs6NiK/k8v1JJ9gngCnRgLkeXVGa5Hc6ad2lH0bEc5LOJp2otiTVKlcprTY73xcN/ac0z+ZcUufxxqQmxiXAVRGxMCeG/UhNuwuBbzdTTXKgcFKokGsL04DDI2Jps3Ukl1lOALNIE9Uqb/G4H7CkyUbpkE9Ax5AmKD5CuovX50nt21tExL8kfYDUlPT+iKj75LSBomLOykjgc6ShyJNJyffDpDlBP42Ih/K/y17AX6OJJmIOJE4KFXLV9Yekk9dVTgjFknQIqa9mQjTnJDQkjSWtvf9ATlyHkpZg/nNEXCDpPNLw2ceAN5AmrDV0SHIZ5TkTQyPdFe21bUk1N9OdQrpD3STg1aR7JUwty7yKsnNS6CDXFtaPiD82OpaBKE9Ym0laNK7mN5fpi3xCegJYTqoRtADTSTWGN5Bu5XiBpO1JgzSWx7qbK1kv5cmhPyeN8rsZuAm4JiJOz69vQWpGGk5alXh1pJnZVgcefdRBM84CHkgiYpbSapY7kk4ITSMinswd478lDXfckbSG1HPAi8D2ufZwsfuZqiNpDGngwXcj4vJctj9wvaSnIuKsSGtczSP1742JiPkNC/hlyDUFa5hmGznSJs+gnkZKCluxrnNzV2ApsEdEPNO4CMtL0gnAuIg4NfcPtN1Q6S2kZqPvkWpqJwIfi7QasdWRk4JZJ5TuUTEV2C3SwoObAoOBjTwPoXp5VYBvkJqOjiKN5hpHak7aizQvaEPg55GWTrc6c/ORWSciLTTYCsyVNCGa+Cb2JXMnaTmZM0nDSs8B7iXd62Ej0q1BV7aNUGvGmuRA56Rg1oWI+I3STYt+K2kXj0brv0irCZ+tdD/z9ol+knYk3eJ2E2Bl3tcJoQHcfGTWA0nDPPqlNvIw8ANJS6if3rawoDWOk4KZNUROCG0L350T+b4Z1lhOCmbWMDkxbB4R/3QfQnNwUjAzs3ZNuR65mZk1hpOCmZm1c1IwM7N2TgpmZtbOScHMzNo5KZiZWTsnBTMza/f/AWhzluuD1IGmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot confusion matrix\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(cm)\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(classes)), classes, rotation=45)\n",
    "plt.yticks(np.arange(len(classes)), classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's test on some of the validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "label_of_image: Happy\n",
      "C:/Users/Dell/Desktop/MY_REPO/Data-Science-Projects/Detecting of human emotions on images and webcam/validation/Happy/696.jpg\n",
      "3\n",
      "label_of_image: Neutral\n",
      "C:/Users/Dell/Desktop/MY_REPO/Data-Science-Projects/Detecting of human emotions on images and webcam/validation/Neutral/201.jpg\n",
      "4\n",
      "label_of_image: Sad\n",
      "C:/Users/Dell/Desktop/MY_REPO/Data-Science-Projects/Detecting of human emotions on images and webcam/validation/Sad/347.jpg\n",
      "5\n",
      "label_of_image: Surprise\n",
      "C:/Users/Dell/Desktop/MY_REPO/Data-Science-Projects/Detecting of human emotions on images and webcam/validation/Surprise/94.jpg\n",
      "0\n",
      "label_of_image: Angry\n",
      "C:/Users/Dell/Desktop/MY_REPO/Data-Science-Projects/Detecting of human emotions on images and webcam/validation/Angry/126.jpg\n",
      "4\n",
      "label_of_image: Sad\n",
      "C:/Users/Dell/Desktop/MY_REPO/Data-Science-Projects/Detecting of human emotions on images and webcam/validation/Sad/160.jpg\n",
      "4\n",
      "label_of_image: Sad\n",
      "C:/Users/Dell/Desktop/MY_REPO/Data-Science-Projects/Detecting of human emotions on images and webcam/validation/Sad/188.jpg\n",
      "5\n",
      "label_of_image: Surprise\n",
      "C:/Users/Dell/Desktop/MY_REPO/Data-Science-Projects/Detecting of human emotions on images and webcam/validation/Surprise/133.jpg\n",
      "4\n",
      "label_of_image: Sad\n",
      "C:/Users/Dell/Desktop/MY_REPO/Data-Science-Projects/Detecting of human emotions on images and webcam/validation/Sad/186.jpg\n",
      "3\n",
      "label_of_image: Neutral\n",
      "C:/Users/Dell/Desktop/MY_REPO/Data-Science-Projects/Detecting of human emotions on images and webcam/validation/Neutral/114.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from keras.preprocessing import image\n",
    "\n",
    "\n",
    "def draw_test(name, pred, im, true_label):\n",
    "    Black = [0,0,0]\n",
    "    expanded_image = cv2.copyMakeBorder(im, 160, 0,0 , 300, cv2.BORDER_CONSTANT, value=Black)\n",
    "    cv2.putText(expanded_image, 'Predicted -' +str(pred), (20,60), cv2.FONT_HERSHEY_SCRIPT_SIMPLEX, 1, (0,0,255), 2)\n",
    "    cv2.putText(expanded_image, 'true -' +str(true_label), (20, 120), cv2.FONT_HERSHEY_SCRIPT_SIMPLEX, 1, (0,0,255), 2)\n",
    "    cv2.imshow(name, expanded_image)\n",
    "    cv2.imwrite('predicted_image{}.jpg'.format(pred), expanded_image)\n",
    "    \n",
    "    \n",
    "    \n",
    "def random_img_from_val_dir(path):\n",
    "    #folders inisde validation dir \n",
    "    folder_inside_val = os.listdir(path)\n",
    "    \n",
    "    #getting random folder number\n",
    "    n_folders = 6\n",
    "    random_folder_number = random.choice(list(range(0, n_folders)))\n",
    "    print(random_folder_number)\n",
    "    print('label_of_image:' ,classes[random_folder_number])\n",
    "    \n",
    "    #listing files inside the random folder\n",
    "    files_inside_rf = os.listdir(os.path.join(path, folder_inside_val[random_folder_number]))\n",
    "    \n",
    "    #selecting random image inside the selected random folder\n",
    "    random_image_path = os.path.join(path + folder_inside_val[random_folder_number] + '/' +random.choice(files_inside_rf))\n",
    "    print(random_image_path)\n",
    "    \n",
    "    #getting the array of pixels of the random image and convert it into gray\n",
    "    random_image = cv2.imread(random_image_path, 0)\n",
    "    \n",
    "    return random_image, random_image_path, random_folder_number\n",
    "\n",
    "files = []\n",
    "true_labels = []\n",
    "predicted_images = []\n",
    "#predicting images\n",
    "for i in range(0,10):\n",
    "    path_for_random = 'C:/Users/Dell/Desktop/MY_REPO/Data-Science-Projects/Detecting of human emotions on images/validation/'\n",
    "    img, image_path, true_label =random_img_from_val_dir(path_for_random)\n",
    "    files.append(image_path)\n",
    "    true_labels.append(classes[true_label])\n",
    "    x = img/255\n",
    "    x = np.expand_dims(x, axis = 2) # here axis=2 to get the shape of the image (48,48,1)\n",
    "    image = x.reshape(-1, img_rows, img_cols, 1)\n",
    "    #images = np.vstack([x])\n",
    "    predictions_proba = model.predict(image, batch_size=10)\n",
    "    predictions = np.argmax(predictions_proba, axis=1)\n",
    "    predicted_images.append(predictions)\n",
    "    \n",
    "for i in range(0, len(files)):\n",
    "    image = cv2.imread((files[i]))\n",
    "    image = cv2.resize(image, None, fx=3, fy=3, interpolation = cv2.INTER_CUBIC)\n",
    "    draw_test(\"Prediction\", classes[predicted_images[i][0]], image, true_labels[i])\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on own single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array\n",
    "face_classifier = cv2.CascadeClassifier(r'C:\\\\Users\\\\Dell\\\\Desktop\\\\KPML_practice\\\\Master Computer Vision\\Master OpenCV\\\\Haarcascades\\haarcascade_frontalface_default.xml')\n",
    "\n",
    "def detect_face_in_image(img):\n",
    "    #convert to gray scale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #finding faces in image using cascade\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    #if there is no face\n",
    "    if faces is ():\n",
    "        return (0,0,0,0) , np.zeros(shape=(0,0), dtype=np.uint8) , img\n",
    "    \n",
    "    #if there is a face\n",
    "    all_faces = []\n",
    "    rects = []\n",
    "    for (x,y,w,h) in faces:\n",
    "        rects.append((x,y,w,h))\n",
    "        \n",
    "        #draw rectangle on the image using x,y,w,h\n",
    "        cv2.rectangle(img, (x,y), (x+w, y+h), (0,0,255), 2)\n",
    "        \n",
    "        #get the region of interset for the faces in the image\n",
    "        roi_gray = gray[y:y+h , x:x+w]\n",
    "        roi_gray = cv2.resize(roi_gray, (48,48), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        #get the roi of faces\n",
    "        all_faces.append(roi_gray)\n",
    "        \n",
    "    return rects, all_faces, img\n",
    "\n",
    "img = cv2.imread(r'C:\\\\Users\\\\Dell\\\\Desktop\\\\KPML_practice\\\\Master Computer Vision\\\\Master OpenCV\\\\images\\Hillary.jpg')\n",
    "rects, all_faces, image = detect_face_in_image(img)\n",
    "\n",
    "for face in all_faces:\n",
    "    roi = face.astype('float') / 255  # here we are represent it as float since it has to give float values not uint8\n",
    "    roi = img_to_array(roi)\n",
    "    roi = np.reshape(roi, (-1,48,48,1))\n",
    "    \n",
    "    # make a prediction on the ROI, then lookup the class\n",
    "    preds = classifier.predict(roi)[0]\n",
    "    label = classes[preds.argmax()] \n",
    "    \n",
    "    ##Overlay our detected emotion on our pic\n",
    "    cv2.putText(image, label ,(rects[0][0] -10, rects[0][3]-80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)\n",
    "    \n",
    "cv2.imshow(\"Emotion Detector\", image)\n",
    "cv2.imwrite('tested_image.jpg', image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's the end of the code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
