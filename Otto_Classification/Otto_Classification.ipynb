{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "id": "THxALWAltwn1",
    "outputId": "206763b1-9403-4fea-c8c6-8853e111edac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Package 'python-software-properties' has no installation candidate\n",
      "--2018-10-16 19:48:17--  https://launchpad.net/~alessandro-strada/+archive/ubuntu/google-drive-ocamlfuse-beta/+build/15331130/+files/google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb\n",
      "Resolving launchpad.net (launchpad.net)... 91.189.89.223, 91.189.89.222\n",
      "Connecting to launchpad.net (launchpad.net)|91.189.89.223|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://launchpadlibrarian.net/386846978/google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb [following]\n",
      "--2018-10-16 19:48:18--  https://launchpadlibrarian.net/386846978/google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb\n",
      "Resolving launchpadlibrarian.net (launchpadlibrarian.net)... 91.189.89.229, 91.189.89.228\n",
      "Connecting to launchpadlibrarian.net (launchpadlibrarian.net)|91.189.89.229|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1232624 (1.2M) [application/x-debian-package]\n",
      "Saving to: ‘google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb.1’\n",
      "\n",
      "google-drive-ocamlf 100%[===================>]   1.17M   655KB/s    in 1.8s    \n",
      "\n",
      "2018-10-16 19:48:22 (655 KB/s) - ‘google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb.1’ saved [1232624/1232624]\n",
      "\n",
      "(Reading database ... 22309 files and directories currently installed.)\n",
      "Preparing to unpack google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb ...\n",
      "Unpacking google-drive-ocamlfuse (0.7.0-0ubuntu1) over (0.7.0-0ubuntu1) ...\n",
      "Setting up google-drive-ocamlfuse (0.7.0-0ubuntu1) ...\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "0 upgraded, 0 newly installed, 0 to remove and 3 not upgraded.\n",
      "··········\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!wget https://launchpad.net/~alessandro-strada/+archive/ubuntu/google-drive-ocamlfuse-beta/+build/15331130/+files/google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb\n",
    "!dpkg -i google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb\n",
    "!apt-get install -f\n",
    "!apt-get -y install -qq fuse\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "_CkM7bEtuLWO",
    "outputId": "0755fdd1-cfaa-48e4-f9e0-11c920a6cb9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuse: mountpoint is not empty\n",
      "fuse: if you are sure this is safe, use the 'nonempty' mount option\n",
      "Drive/Datasets/Otto_Classification:\n",
      "test.csv  train.csv\n",
      "\n",
      "My:\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p My Drive\n",
    "!google-drive-ocamlfuse My Drive \n",
    "\n",
    "!ls My Drive/Datasets/Otto_Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZiG7CF2zbSQ7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fWm7YN1zbwwe"
   },
   "outputs": [],
   "source": [
    "dataset_train = pd.read_csv('Drive/Datasets/Otto_Classification/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VlEx_i4cezmb"
   },
   "outputs": [],
   "source": [
    "dataset_test = pd.read_csv('Drive/Datasets/Otto_Classification/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "colab_type": "code",
    "id": "e5iGLU9HfS7U",
    "outputId": "964f4956-ddc4-4114-c2df-2e179c882ee8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  feat_9  \\\n",
       "0   1       1       0       0       0       0       0       0       0       0   \n",
       "1   2       0       0       0       0       0       0       0       1       0   \n",
       "2   3       0       0       0       0       0       0       0       1       0   \n",
       "3   4       1       0       0       1       6       1       5       0       0   \n",
       "4   5       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "    ...     feat_85  feat_86  feat_87  feat_88  feat_89  feat_90  feat_91  \\\n",
       "0   ...           1        0        0        0        0        0        0   \n",
       "1   ...           0        0        0        0        0        0        0   \n",
       "2   ...           0        0        0        0        0        0        0   \n",
       "3   ...           0        1        2        0        0        0        0   \n",
       "4   ...           1        0        0        0        0        1        0   \n",
       "\n",
       "   feat_92  feat_93   target  \n",
       "0        0        0  Class_1  \n",
       "1        0        0  Class_1  \n",
       "2        0        0  Class_1  \n",
       "3        0        0  Class_1  \n",
       "4        0        0  Class_1  \n",
       "\n",
       "[5 rows x 95 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "colab_type": "code",
    "id": "IADB99VucDtu",
    "outputId": "f5f2f19b-a969-4f1a-9f70-0e31d8231bb9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_84</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.00000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61878.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>30939.500000</td>\n",
       "      <td>0.38668</td>\n",
       "      <td>0.263066</td>\n",
       "      <td>0.901467</td>\n",
       "      <td>0.779081</td>\n",
       "      <td>0.071043</td>\n",
       "      <td>0.025696</td>\n",
       "      <td>0.193704</td>\n",
       "      <td>0.662433</td>\n",
       "      <td>1.011296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070752</td>\n",
       "      <td>0.532306</td>\n",
       "      <td>1.128576</td>\n",
       "      <td>0.393549</td>\n",
       "      <td>0.874915</td>\n",
       "      <td>0.457772</td>\n",
       "      <td>0.812421</td>\n",
       "      <td>0.264941</td>\n",
       "      <td>0.380119</td>\n",
       "      <td>0.126135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17862.784315</td>\n",
       "      <td>1.52533</td>\n",
       "      <td>1.252073</td>\n",
       "      <td>2.934818</td>\n",
       "      <td>2.788005</td>\n",
       "      <td>0.438902</td>\n",
       "      <td>0.215333</td>\n",
       "      <td>1.030102</td>\n",
       "      <td>2.255770</td>\n",
       "      <td>3.474822</td>\n",
       "      <td>...</td>\n",
       "      <td>1.151460</td>\n",
       "      <td>1.900438</td>\n",
       "      <td>2.681554</td>\n",
       "      <td>1.575455</td>\n",
       "      <td>2.115466</td>\n",
       "      <td>1.527385</td>\n",
       "      <td>4.597804</td>\n",
       "      <td>2.045646</td>\n",
       "      <td>0.982385</td>\n",
       "      <td>1.201720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>15470.250000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>30939.500000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>46408.750000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>61878.000000</td>\n",
       "      <td>61.00000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>87.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id       feat_1        feat_2        feat_3        feat_4  \\\n",
       "count  61878.000000  61878.00000  61878.000000  61878.000000  61878.000000   \n",
       "mean   30939.500000      0.38668      0.263066      0.901467      0.779081   \n",
       "std    17862.784315      1.52533      1.252073      2.934818      2.788005   \n",
       "min        1.000000      0.00000      0.000000      0.000000      0.000000   \n",
       "25%    15470.250000      0.00000      0.000000      0.000000      0.000000   \n",
       "50%    30939.500000      0.00000      0.000000      0.000000      0.000000   \n",
       "75%    46408.750000      0.00000      0.000000      0.000000      0.000000   \n",
       "max    61878.000000     61.00000     51.000000     64.000000     70.000000   \n",
       "\n",
       "             feat_5        feat_6        feat_7        feat_8        feat_9  \\\n",
       "count  61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       0.071043      0.025696      0.193704      0.662433      1.011296   \n",
       "std        0.438902      0.215333      1.030102      2.255770      3.474822   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      1.000000      0.000000   \n",
       "max       19.000000     10.000000     38.000000     76.000000     43.000000   \n",
       "\n",
       "           ...            feat_84       feat_85       feat_86       feat_87  \\\n",
       "count      ...       61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       ...           0.070752      0.532306      1.128576      0.393549   \n",
       "std        ...           1.151460      1.900438      2.681554      1.575455   \n",
       "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "25%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "50%        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "75%        ...           0.000000      0.000000      1.000000      0.000000   \n",
       "max        ...          76.000000     55.000000     65.000000     67.000000   \n",
       "\n",
       "            feat_88       feat_89       feat_90       feat_91       feat_92  \\\n",
       "count  61878.000000  61878.000000  61878.000000  61878.000000  61878.000000   \n",
       "mean       0.874915      0.457772      0.812421      0.264941      0.380119   \n",
       "std        2.115466      1.527385      4.597804      2.045646      0.982385   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        1.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max       30.000000     61.000000    130.000000     52.000000     19.000000   \n",
       "\n",
       "            feat_93  \n",
       "count  61878.000000  \n",
       "mean       0.126135  \n",
       "std        1.201720  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max       87.000000  \n",
       "\n",
       "[8 rows x 94 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "colab_type": "code",
    "id": "-rqlgcBcfY-U",
    "outputId": "ef226f98-2390-4333-b939-a5ce20a2be4f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_84</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  feat_9  \\\n",
       "0   1       0       0       0       0       0       0       0       0       0   \n",
       "1   2       2       2      14      16       0       0       0       0       0   \n",
       "2   3       0       1      12       1       0       0       0       0       0   \n",
       "3   4       0       0       0       1       0       0       0       0       0   \n",
       "4   5       1       0       0       1       0       0       1       2       0   \n",
       "\n",
       "    ...     feat_84  feat_85  feat_86  feat_87  feat_88  feat_89  feat_90  \\\n",
       "0   ...           0        0       11        1       20        0        0   \n",
       "1   ...           0        0        0        0        0        4        0   \n",
       "2   ...           0        0        0        0        2        0        0   \n",
       "3   ...           0        3        1        0        0        0        0   \n",
       "4   ...           0        0        0        0        0        0        0   \n",
       "\n",
       "   feat_91  feat_92  feat_93  \n",
       "0        0        0        0  \n",
       "1        0        2        0  \n",
       "2        0        0        1  \n",
       "3        0        0        0  \n",
       "4        9        0        0  \n",
       "\n",
       "[5 rows x 94 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "colab_type": "code",
    "id": "Vb9fgu9IcGMo",
    "outputId": "3b71ca5d-1ea6-4f49-acc1-f8fef33ea97c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_84</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>144368.000000</td>\n",
       "      <td>144368.000000</td>\n",
       "      <td>144368.000000</td>\n",
       "      <td>144368.000000</td>\n",
       "      <td>144368.000000</td>\n",
       "      <td>144368.000000</td>\n",
       "      <td>144368.000000</td>\n",
       "      <td>144368.000000</td>\n",
       "      <td>144368.000000</td>\n",
       "      <td>144368.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>144368.000000</td>\n",
       "      <td>144368.000000</td>\n",
       "      <td>144368.000000</td>\n",
       "      <td>144368.000000</td>\n",
       "      <td>144368.000000</td>\n",
       "      <td>144368.000000</td>\n",
       "      <td>144368.000000</td>\n",
       "      <td>144368.000000</td>\n",
       "      <td>144368.000000</td>\n",
       "      <td>144368.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>72184.500000</td>\n",
       "      <td>0.386201</td>\n",
       "      <td>0.263597</td>\n",
       "      <td>0.899819</td>\n",
       "      <td>0.780727</td>\n",
       "      <td>0.071498</td>\n",
       "      <td>0.026439</td>\n",
       "      <td>0.200169</td>\n",
       "      <td>0.667378</td>\n",
       "      <td>1.035271</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074989</td>\n",
       "      <td>0.538485</td>\n",
       "      <td>1.128782</td>\n",
       "      <td>0.405249</td>\n",
       "      <td>0.875526</td>\n",
       "      <td>0.473284</td>\n",
       "      <td>0.814010</td>\n",
       "      <td>0.271161</td>\n",
       "      <td>0.388348</td>\n",
       "      <td>0.132675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>41675.596169</td>\n",
       "      <td>1.468882</td>\n",
       "      <td>1.261908</td>\n",
       "      <td>2.949106</td>\n",
       "      <td>2.846181</td>\n",
       "      <td>0.428568</td>\n",
       "      <td>0.228354</td>\n",
       "      <td>1.069235</td>\n",
       "      <td>2.286832</td>\n",
       "      <td>3.548618</td>\n",
       "      <td>...</td>\n",
       "      <td>1.288595</td>\n",
       "      <td>1.906121</td>\n",
       "      <td>2.682511</td>\n",
       "      <td>1.631566</td>\n",
       "      <td>2.090288</td>\n",
       "      <td>1.617853</td>\n",
       "      <td>4.603653</td>\n",
       "      <td>2.073627</td>\n",
       "      <td>1.006935</td>\n",
       "      <td>1.302695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>36092.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>72184.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>108276.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>144368.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>91.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id         feat_1         feat_2         feat_3  \\\n",
       "count  144368.000000  144368.000000  144368.000000  144368.000000   \n",
       "mean    72184.500000       0.386201       0.263597       0.899819   \n",
       "std     41675.596169       1.468882       1.261908       2.949106   \n",
       "min         1.000000       0.000000       0.000000       0.000000   \n",
       "25%     36092.750000       0.000000       0.000000       0.000000   \n",
       "50%     72184.500000       0.000000       0.000000       0.000000   \n",
       "75%    108276.250000       0.000000       0.000000       0.000000   \n",
       "max    144368.000000      64.000000      45.000000      84.000000   \n",
       "\n",
       "              feat_4         feat_5         feat_6         feat_7  \\\n",
       "count  144368.000000  144368.000000  144368.000000  144368.000000   \n",
       "mean        0.780727       0.071498       0.026439       0.200169   \n",
       "std         2.846181       0.428568       0.228354       1.069235   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max        82.000000      14.000000      11.000000      44.000000   \n",
       "\n",
       "              feat_8         feat_9      ...              feat_84  \\\n",
       "count  144368.000000  144368.000000      ...        144368.000000   \n",
       "mean        0.667378       1.035271      ...             0.074989   \n",
       "std         2.286832       3.548618      ...             1.288595   \n",
       "min         0.000000       0.000000      ...             0.000000   \n",
       "25%         0.000000       0.000000      ...             0.000000   \n",
       "50%         0.000000       0.000000      ...             0.000000   \n",
       "75%         1.000000       0.000000      ...             0.000000   \n",
       "max       100.000000      47.000000      ...           132.000000   \n",
       "\n",
       "             feat_85        feat_86        feat_87        feat_88  \\\n",
       "count  144368.000000  144368.000000  144368.000000  144368.000000   \n",
       "mean        0.538485       1.128782       0.405249       0.875526   \n",
       "std         1.906121       2.682511       1.631566       2.090288   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       1.000000       0.000000       1.000000   \n",
       "max        56.000000      73.000000      54.000000      37.000000   \n",
       "\n",
       "             feat_89        feat_90        feat_91        feat_92  \\\n",
       "count  144368.000000  144368.000000  144368.000000  144368.000000   \n",
       "mean        0.473284       0.814010       0.271161       0.388348   \n",
       "std         1.617853       4.603653       2.073627       1.006935   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max        62.000000     119.000000      74.000000      22.000000   \n",
       "\n",
       "             feat_93  \n",
       "count  144368.000000  \n",
       "mean        0.132675  \n",
       "std         1.302695  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max        91.000000  \n",
       "\n",
       "[8 rows x 94 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1835
    },
    "colab_type": "code",
    "id": "Hk1uPzSLe_Tq",
    "outputId": "4dc6dc83-32a1-4e89-e612-6f1684987b81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 61878 entries, 0 to 61877\n",
      "Data columns (total 95 columns):\n",
      "id         61878 non-null int64\n",
      "feat_1     61878 non-null int64\n",
      "feat_2     61878 non-null int64\n",
      "feat_3     61878 non-null int64\n",
      "feat_4     61878 non-null int64\n",
      "feat_5     61878 non-null int64\n",
      "feat_6     61878 non-null int64\n",
      "feat_7     61878 non-null int64\n",
      "feat_8     61878 non-null int64\n",
      "feat_9     61878 non-null int64\n",
      "feat_10    61878 non-null int64\n",
      "feat_11    61878 non-null int64\n",
      "feat_12    61878 non-null int64\n",
      "feat_13    61878 non-null int64\n",
      "feat_14    61878 non-null int64\n",
      "feat_15    61878 non-null int64\n",
      "feat_16    61878 non-null int64\n",
      "feat_17    61878 non-null int64\n",
      "feat_18    61878 non-null int64\n",
      "feat_19    61878 non-null int64\n",
      "feat_20    61878 non-null int64\n",
      "feat_21    61878 non-null int64\n",
      "feat_22    61878 non-null int64\n",
      "feat_23    61878 non-null int64\n",
      "feat_24    61878 non-null int64\n",
      "feat_25    61878 non-null int64\n",
      "feat_26    61878 non-null int64\n",
      "feat_27    61878 non-null int64\n",
      "feat_28    61878 non-null int64\n",
      "feat_29    61878 non-null int64\n",
      "feat_30    61878 non-null int64\n",
      "feat_31    61878 non-null int64\n",
      "feat_32    61878 non-null int64\n",
      "feat_33    61878 non-null int64\n",
      "feat_34    61878 non-null int64\n",
      "feat_35    61878 non-null int64\n",
      "feat_36    61878 non-null int64\n",
      "feat_37    61878 non-null int64\n",
      "feat_38    61878 non-null int64\n",
      "feat_39    61878 non-null int64\n",
      "feat_40    61878 non-null int64\n",
      "feat_41    61878 non-null int64\n",
      "feat_42    61878 non-null int64\n",
      "feat_43    61878 non-null int64\n",
      "feat_44    61878 non-null int64\n",
      "feat_45    61878 non-null int64\n",
      "feat_46    61878 non-null int64\n",
      "feat_47    61878 non-null int64\n",
      "feat_48    61878 non-null int64\n",
      "feat_49    61878 non-null int64\n",
      "feat_50    61878 non-null int64\n",
      "feat_51    61878 non-null int64\n",
      "feat_52    61878 non-null int64\n",
      "feat_53    61878 non-null int64\n",
      "feat_54    61878 non-null int64\n",
      "feat_55    61878 non-null int64\n",
      "feat_56    61878 non-null int64\n",
      "feat_57    61878 non-null int64\n",
      "feat_58    61878 non-null int64\n",
      "feat_59    61878 non-null int64\n",
      "feat_60    61878 non-null int64\n",
      "feat_61    61878 non-null int64\n",
      "feat_62    61878 non-null int64\n",
      "feat_63    61878 non-null int64\n",
      "feat_64    61878 non-null int64\n",
      "feat_65    61878 non-null int64\n",
      "feat_66    61878 non-null int64\n",
      "feat_67    61878 non-null int64\n",
      "feat_68    61878 non-null int64\n",
      "feat_69    61878 non-null int64\n",
      "feat_70    61878 non-null int64\n",
      "feat_71    61878 non-null int64\n",
      "feat_72    61878 non-null int64\n",
      "feat_73    61878 non-null int64\n",
      "feat_74    61878 non-null int64\n",
      "feat_75    61878 non-null int64\n",
      "feat_76    61878 non-null int64\n",
      "feat_77    61878 non-null int64\n",
      "feat_78    61878 non-null int64\n",
      "feat_79    61878 non-null int64\n",
      "feat_80    61878 non-null int64\n",
      "feat_81    61878 non-null int64\n",
      "feat_82    61878 non-null int64\n",
      "feat_83    61878 non-null int64\n",
      "feat_84    61878 non-null int64\n",
      "feat_85    61878 non-null int64\n",
      "feat_86    61878 non-null int64\n",
      "feat_87    61878 non-null int64\n",
      "feat_88    61878 non-null int64\n",
      "feat_89    61878 non-null int64\n",
      "feat_90    61878 non-null int64\n",
      "feat_91    61878 non-null int64\n",
      "feat_92    61878 non-null int64\n",
      "feat_93    61878 non-null int64\n",
      "target     61878 non-null object\n",
      "dtypes: int64(94), object(1)\n",
      "memory usage: 44.8+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "bfNVPMZnfeyO",
    "outputId": "0a00a3b7-17db-4be8-af9c-845b2035a8b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6',\n",
       "       'Class_7', 'Class_8', 'Class_9'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "colab_type": "code",
    "id": "kyzNjDWvfjrY",
    "outputId": "4eabd028-c1c9-49aa-9943-c169717bb48e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class_2    16122\n",
       "Class_6    14135\n",
       "Class_8     8464\n",
       "Class_3     8004\n",
       "Class_9     4955\n",
       "Class_7     2839\n",
       "Class_5     2739\n",
       "Class_4     2691\n",
       "Class_1     1929\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1145
    },
    "colab_type": "code",
    "id": "5GA7VeNEfldL",
    "outputId": "39330813-e101-4a98-a01e-2da05c2c5c23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target     0\n",
       "feat_34    0\n",
       "feat_25    0\n",
       "feat_26    0\n",
       "feat_27    0\n",
       "feat_28    0\n",
       "feat_29    0\n",
       "feat_30    0\n",
       "feat_31    0\n",
       "feat_32    0\n",
       "feat_33    0\n",
       "feat_35    0\n",
       "feat_23    0\n",
       "feat_36    0\n",
       "feat_37    0\n",
       "feat_38    0\n",
       "feat_39    0\n",
       "feat_40    0\n",
       "feat_41    0\n",
       "feat_42    0\n",
       "feat_43    0\n",
       "feat_44    0\n",
       "feat_24    0\n",
       "feat_22    0\n",
       "feat_46    0\n",
       "feat_10    0\n",
       "feat_1     0\n",
       "feat_2     0\n",
       "feat_3     0\n",
       "feat_4     0\n",
       "          ..\n",
       "feat_88    0\n",
       "feat_89    0\n",
       "feat_90    0\n",
       "feat_91    0\n",
       "feat_92    0\n",
       "feat_72    0\n",
       "feat_70    0\n",
       "feat_48    0\n",
       "feat_58    0\n",
       "feat_49    0\n",
       "feat_50    0\n",
       "feat_51    0\n",
       "feat_52    0\n",
       "feat_53    0\n",
       "feat_54    0\n",
       "feat_55    0\n",
       "feat_56    0\n",
       "feat_57    0\n",
       "feat_59    0\n",
       "feat_69    0\n",
       "feat_60    0\n",
       "feat_61    0\n",
       "feat_62    0\n",
       "feat_63    0\n",
       "feat_64    0\n",
       "feat_65    0\n",
       "feat_66    0\n",
       "feat_67    0\n",
       "feat_68    0\n",
       "id         0\n",
       "Length: 95, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the null values\n",
    "dataset_train.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1145
    },
    "colab_type": "code",
    "id": "vkwYyLN4fvPB",
    "outputId": "52f11d46-5996-4cca-e0b1-def68088aaff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feat_93    0\n",
       "feat_34    0\n",
       "feat_25    0\n",
       "feat_26    0\n",
       "feat_27    0\n",
       "feat_28    0\n",
       "feat_29    0\n",
       "feat_30    0\n",
       "feat_31    0\n",
       "feat_32    0\n",
       "feat_33    0\n",
       "feat_35    0\n",
       "feat_23    0\n",
       "feat_36    0\n",
       "feat_37    0\n",
       "feat_38    0\n",
       "feat_39    0\n",
       "feat_40    0\n",
       "feat_41    0\n",
       "feat_42    0\n",
       "feat_43    0\n",
       "feat_44    0\n",
       "feat_24    0\n",
       "feat_22    0\n",
       "feat_92    0\n",
       "feat_10    0\n",
       "feat_1     0\n",
       "feat_2     0\n",
       "feat_3     0\n",
       "feat_4     0\n",
       "          ..\n",
       "feat_86    0\n",
       "feat_87    0\n",
       "feat_88    0\n",
       "feat_89    0\n",
       "feat_90    0\n",
       "feat_91    0\n",
       "feat_71    0\n",
       "feat_70    0\n",
       "feat_69    0\n",
       "feat_68    0\n",
       "feat_49    0\n",
       "feat_50    0\n",
       "feat_51    0\n",
       "feat_52    0\n",
       "feat_53    0\n",
       "feat_54    0\n",
       "feat_55    0\n",
       "feat_56    0\n",
       "feat_57    0\n",
       "feat_58    0\n",
       "feat_59    0\n",
       "feat_60    0\n",
       "feat_61    0\n",
       "feat_62    0\n",
       "feat_63    0\n",
       "feat_64    0\n",
       "feat_65    0\n",
       "feat_66    0\n",
       "feat_67    0\n",
       "id         0\n",
       "Length: 94, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lLRgG51Yf2sE"
   },
   "source": [
    "We can see that there are no missing values in both training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2342
    },
    "colab_type": "code",
    "id": "E-Kp6MzHfyzX",
    "outputId": "a0193855-1147-4c5a-f727-ad79092d7152"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_84</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.070691</td>\n",
       "      <td>0.185463</td>\n",
       "      <td>0.138980</td>\n",
       "      <td>1.460200e-01</td>\n",
       "      <td>0.030707</td>\n",
       "      <td>0.033410</td>\n",
       "      <td>0.136501</td>\n",
       "      <td>0.197418</td>\n",
       "      <td>-0.156082</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008192</td>\n",
       "      <td>-0.084507</td>\n",
       "      <td>-0.096484</td>\n",
       "      <td>0.097087</td>\n",
       "      <td>-0.215878</td>\n",
       "      <td>0.111519</td>\n",
       "      <td>0.188895</td>\n",
       "      <td>0.139078</td>\n",
       "      <td>0.131737</td>\n",
       "      <td>0.047944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_1</th>\n",
       "      <td>0.070691</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031332</td>\n",
       "      <td>-0.027807</td>\n",
       "      <td>-2.752941e-02</td>\n",
       "      <td>0.042973</td>\n",
       "      <td>0.043603</td>\n",
       "      <td>0.298952</td>\n",
       "      <td>0.056321</td>\n",
       "      <td>-0.032285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049634</td>\n",
       "      <td>-0.008739</td>\n",
       "      <td>0.107947</td>\n",
       "      <td>0.089374</td>\n",
       "      <td>0.020830</td>\n",
       "      <td>0.096851</td>\n",
       "      <td>0.010310</td>\n",
       "      <td>0.037264</td>\n",
       "      <td>0.054777</td>\n",
       "      <td>0.081783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_2</th>\n",
       "      <td>0.185463</td>\n",
       "      <td>0.031332</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082573</td>\n",
       "      <td>1.349870e-01</td>\n",
       "      <td>0.020926</td>\n",
       "      <td>0.041343</td>\n",
       "      <td>0.222386</td>\n",
       "      <td>0.019815</td>\n",
       "      <td>-0.025630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009845</td>\n",
       "      <td>-0.006764</td>\n",
       "      <td>-0.039090</td>\n",
       "      <td>0.047451</td>\n",
       "      <td>-0.047035</td>\n",
       "      <td>0.105527</td>\n",
       "      <td>0.515022</td>\n",
       "      <td>0.026383</td>\n",
       "      <td>-0.008219</td>\n",
       "      <td>0.054593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_3</th>\n",
       "      <td>0.138980</td>\n",
       "      <td>-0.027807</td>\n",
       "      <td>0.082573</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.835232e-01</td>\n",
       "      <td>0.010880</td>\n",
       "      <td>0.004288</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>-0.053462</td>\n",
       "      <td>-0.063551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011159</td>\n",
       "      <td>-0.048626</td>\n",
       "      <td>-0.096093</td>\n",
       "      <td>-0.009838</td>\n",
       "      <td>-0.082336</td>\n",
       "      <td>0.174781</td>\n",
       "      <td>-0.015068</td>\n",
       "      <td>-0.012417</td>\n",
       "      <td>0.066921</td>\n",
       "      <td>0.006814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_4</th>\n",
       "      <td>0.146020</td>\n",
       "      <td>-0.027529</td>\n",
       "      <td>0.134987</td>\n",
       "      <td>0.583523</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.017290</td>\n",
       "      <td>0.014059</td>\n",
       "      <td>0.014490</td>\n",
       "      <td>-0.046184</td>\n",
       "      <td>-0.046250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005684</td>\n",
       "      <td>-0.033153</td>\n",
       "      <td>-0.071029</td>\n",
       "      <td>0.005055</td>\n",
       "      <td>-0.067484</td>\n",
       "      <td>0.183715</td>\n",
       "      <td>0.009454</td>\n",
       "      <td>-0.010312</td>\n",
       "      <td>0.087631</td>\n",
       "      <td>0.015746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_5</th>\n",
       "      <td>0.030707</td>\n",
       "      <td>0.042973</td>\n",
       "      <td>0.020926</td>\n",
       "      <td>0.010880</td>\n",
       "      <td>1.729026e-02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.145355</td>\n",
       "      <td>0.075047</td>\n",
       "      <td>0.035861</td>\n",
       "      <td>-0.024708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.467329</td>\n",
       "      <td>0.034062</td>\n",
       "      <td>0.013879</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>-0.019201</td>\n",
       "      <td>0.119951</td>\n",
       "      <td>0.004842</td>\n",
       "      <td>0.012012</td>\n",
       "      <td>0.065331</td>\n",
       "      <td>0.002038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_6</th>\n",
       "      <td>0.033410</td>\n",
       "      <td>0.043603</td>\n",
       "      <td>0.041343</td>\n",
       "      <td>0.004288</td>\n",
       "      <td>1.405895e-02</td>\n",
       "      <td>0.145355</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.088014</td>\n",
       "      <td>0.012867</td>\n",
       "      <td>-0.009373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177777</td>\n",
       "      <td>0.004290</td>\n",
       "      <td>0.010455</td>\n",
       "      <td>0.015256</td>\n",
       "      <td>-0.015437</td>\n",
       "      <td>0.035042</td>\n",
       "      <td>0.054034</td>\n",
       "      <td>0.012465</td>\n",
       "      <td>0.015479</td>\n",
       "      <td>0.008521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_7</th>\n",
       "      <td>0.136501</td>\n",
       "      <td>0.298952</td>\n",
       "      <td>0.222386</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>1.448981e-02</td>\n",
       "      <td>0.075047</td>\n",
       "      <td>0.088014</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.038121</td>\n",
       "      <td>-0.027146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062634</td>\n",
       "      <td>0.037874</td>\n",
       "      <td>-0.009169</td>\n",
       "      <td>0.089574</td>\n",
       "      <td>-0.033646</td>\n",
       "      <td>0.063511</td>\n",
       "      <td>0.129578</td>\n",
       "      <td>0.068506</td>\n",
       "      <td>-0.032261</td>\n",
       "      <td>0.034912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_8</th>\n",
       "      <td>0.197418</td>\n",
       "      <td>0.056321</td>\n",
       "      <td>0.019815</td>\n",
       "      <td>-0.053462</td>\n",
       "      <td>-4.618407e-02</td>\n",
       "      <td>0.035861</td>\n",
       "      <td>0.012867</td>\n",
       "      <td>0.038121</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.039281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005064</td>\n",
       "      <td>-0.003416</td>\n",
       "      <td>-0.029395</td>\n",
       "      <td>0.059929</td>\n",
       "      <td>-0.050931</td>\n",
       "      <td>0.007974</td>\n",
       "      <td>0.026807</td>\n",
       "      <td>0.095990</td>\n",
       "      <td>0.013608</td>\n",
       "      <td>0.005131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_9</th>\n",
       "      <td>-0.156082</td>\n",
       "      <td>-0.032285</td>\n",
       "      <td>-0.025630</td>\n",
       "      <td>-0.063551</td>\n",
       "      <td>-4.624977e-02</td>\n",
       "      <td>-0.024708</td>\n",
       "      <td>-0.009373</td>\n",
       "      <td>-0.027146</td>\n",
       "      <td>-0.039281</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013569</td>\n",
       "      <td>-0.031462</td>\n",
       "      <td>-0.019144</td>\n",
       "      <td>-0.016925</td>\n",
       "      <td>0.001160</td>\n",
       "      <td>-0.019147</td>\n",
       "      <td>-0.020698</td>\n",
       "      <td>-0.014742</td>\n",
       "      <td>-0.069707</td>\n",
       "      <td>-0.006038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_10</th>\n",
       "      <td>0.096127</td>\n",
       "      <td>0.097776</td>\n",
       "      <td>0.051925</td>\n",
       "      <td>0.036944</td>\n",
       "      <td>5.951396e-02</td>\n",
       "      <td>0.091324</td>\n",
       "      <td>0.041940</td>\n",
       "      <td>0.194258</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.024323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017939</td>\n",
       "      <td>0.086758</td>\n",
       "      <td>0.159447</td>\n",
       "      <td>0.077421</td>\n",
       "      <td>0.054635</td>\n",
       "      <td>0.061498</td>\n",
       "      <td>0.049908</td>\n",
       "      <td>0.024025</td>\n",
       "      <td>-0.006869</td>\n",
       "      <td>0.041316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_11</th>\n",
       "      <td>0.179164</td>\n",
       "      <td>-0.042928</td>\n",
       "      <td>0.118534</td>\n",
       "      <td>0.596243</td>\n",
       "      <td>3.894092e-01</td>\n",
       "      <td>0.004882</td>\n",
       "      <td>0.014504</td>\n",
       "      <td>0.012418</td>\n",
       "      <td>-0.065923</td>\n",
       "      <td>-0.075820</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017724</td>\n",
       "      <td>-0.074293</td>\n",
       "      <td>-0.123339</td>\n",
       "      <td>-0.032969</td>\n",
       "      <td>-0.114491</td>\n",
       "      <td>0.137374</td>\n",
       "      <td>0.045074</td>\n",
       "      <td>-0.029511</td>\n",
       "      <td>0.013179</td>\n",
       "      <td>0.003326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_12</th>\n",
       "      <td>0.079170</td>\n",
       "      <td>0.056934</td>\n",
       "      <td>0.090153</td>\n",
       "      <td>0.050037</td>\n",
       "      <td>5.743356e-02</td>\n",
       "      <td>0.036668</td>\n",
       "      <td>0.028588</td>\n",
       "      <td>0.056230</td>\n",
       "      <td>0.091424</td>\n",
       "      <td>-0.021885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009807</td>\n",
       "      <td>0.019283</td>\n",
       "      <td>-0.007214</td>\n",
       "      <td>0.016089</td>\n",
       "      <td>-0.024324</td>\n",
       "      <td>0.082220</td>\n",
       "      <td>0.062721</td>\n",
       "      <td>0.063965</td>\n",
       "      <td>0.063922</td>\n",
       "      <td>0.012722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_13</th>\n",
       "      <td>0.184629</td>\n",
       "      <td>0.139254</td>\n",
       "      <td>0.157467</td>\n",
       "      <td>0.013870</td>\n",
       "      <td>2.897317e-02</td>\n",
       "      <td>0.059081</td>\n",
       "      <td>0.036293</td>\n",
       "      <td>0.199142</td>\n",
       "      <td>0.095365</td>\n",
       "      <td>-0.040164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023221</td>\n",
       "      <td>0.002594</td>\n",
       "      <td>0.004850</td>\n",
       "      <td>0.093870</td>\n",
       "      <td>-0.036259</td>\n",
       "      <td>0.062990</td>\n",
       "      <td>0.107722</td>\n",
       "      <td>0.044338</td>\n",
       "      <td>0.071953</td>\n",
       "      <td>0.038989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_14</th>\n",
       "      <td>-0.346308</td>\n",
       "      <td>0.063517</td>\n",
       "      <td>-0.070057</td>\n",
       "      <td>-0.111105</td>\n",
       "      <td>-9.921490e-02</td>\n",
       "      <td>-0.037607</td>\n",
       "      <td>-0.027350</td>\n",
       "      <td>-0.044671</td>\n",
       "      <td>-0.061799</td>\n",
       "      <td>-0.110188</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027058</td>\n",
       "      <td>-0.021455</td>\n",
       "      <td>0.145787</td>\n",
       "      <td>-0.020229</td>\n",
       "      <td>0.323089</td>\n",
       "      <td>-0.038881</td>\n",
       "      <td>-0.060240</td>\n",
       "      <td>-0.038444</td>\n",
       "      <td>-0.040133</td>\n",
       "      <td>-0.018127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_15</th>\n",
       "      <td>-0.245562</td>\n",
       "      <td>-0.045738</td>\n",
       "      <td>-0.048798</td>\n",
       "      <td>-0.065285</td>\n",
       "      <td>-5.122155e-02</td>\n",
       "      <td>-0.007000</td>\n",
       "      <td>-0.018328</td>\n",
       "      <td>-0.035721</td>\n",
       "      <td>-0.056960</td>\n",
       "      <td>0.009858</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009311</td>\n",
       "      <td>0.246847</td>\n",
       "      <td>-0.002529</td>\n",
       "      <td>-0.023191</td>\n",
       "      <td>0.010840</td>\n",
       "      <td>0.029547</td>\n",
       "      <td>-0.046616</td>\n",
       "      <td>-0.034402</td>\n",
       "      <td>-0.018206</td>\n",
       "      <td>-0.020369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_16</th>\n",
       "      <td>-0.004622</td>\n",
       "      <td>0.027086</td>\n",
       "      <td>0.108046</td>\n",
       "      <td>0.221426</td>\n",
       "      <td>2.110780e-01</td>\n",
       "      <td>0.062877</td>\n",
       "      <td>0.021934</td>\n",
       "      <td>0.043957</td>\n",
       "      <td>-0.004659</td>\n",
       "      <td>-0.082664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035211</td>\n",
       "      <td>0.110850</td>\n",
       "      <td>0.003610</td>\n",
       "      <td>0.077770</td>\n",
       "      <td>-0.007257</td>\n",
       "      <td>0.248364</td>\n",
       "      <td>0.016863</td>\n",
       "      <td>0.048494</td>\n",
       "      <td>0.210499</td>\n",
       "      <td>0.031467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_17</th>\n",
       "      <td>0.122884</td>\n",
       "      <td>0.053004</td>\n",
       "      <td>0.074902</td>\n",
       "      <td>-0.023093</td>\n",
       "      <td>-7.553867e-03</td>\n",
       "      <td>0.062197</td>\n",
       "      <td>0.015488</td>\n",
       "      <td>0.127245</td>\n",
       "      <td>0.173912</td>\n",
       "      <td>-0.028709</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.015559</td>\n",
       "      <td>0.049102</td>\n",
       "      <td>0.214221</td>\n",
       "      <td>-0.034139</td>\n",
       "      <td>0.035390</td>\n",
       "      <td>0.045218</td>\n",
       "      <td>0.088508</td>\n",
       "      <td>-0.006538</td>\n",
       "      <td>0.056695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_18</th>\n",
       "      <td>0.189280</td>\n",
       "      <td>0.084856</td>\n",
       "      <td>0.242716</td>\n",
       "      <td>0.115655</td>\n",
       "      <td>2.148952e-01</td>\n",
       "      <td>0.052186</td>\n",
       "      <td>0.048710</td>\n",
       "      <td>0.098972</td>\n",
       "      <td>0.087777</td>\n",
       "      <td>-0.043642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028752</td>\n",
       "      <td>-0.001555</td>\n",
       "      <td>-0.029295</td>\n",
       "      <td>0.126886</td>\n",
       "      <td>-0.035981</td>\n",
       "      <td>0.247462</td>\n",
       "      <td>0.094336</td>\n",
       "      <td>0.037275</td>\n",
       "      <td>0.126640</td>\n",
       "      <td>0.058100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_19</th>\n",
       "      <td>0.127893</td>\n",
       "      <td>0.002302</td>\n",
       "      <td>0.176655</td>\n",
       "      <td>-0.012228</td>\n",
       "      <td>-3.519107e-07</td>\n",
       "      <td>-0.008556</td>\n",
       "      <td>0.038493</td>\n",
       "      <td>0.058071</td>\n",
       "      <td>0.019387</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002847</td>\n",
       "      <td>-0.008292</td>\n",
       "      <td>-0.014560</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>-0.018485</td>\n",
       "      <td>0.011116</td>\n",
       "      <td>0.450925</td>\n",
       "      <td>0.004085</td>\n",
       "      <td>-0.027662</td>\n",
       "      <td>0.014243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_20</th>\n",
       "      <td>0.213630</td>\n",
       "      <td>0.070511</td>\n",
       "      <td>0.449160</td>\n",
       "      <td>-0.011069</td>\n",
       "      <td>4.465657e-02</td>\n",
       "      <td>0.046200</td>\n",
       "      <td>0.057813</td>\n",
       "      <td>0.364972</td>\n",
       "      <td>0.062595</td>\n",
       "      <td>-0.023397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001723</td>\n",
       "      <td>0.084570</td>\n",
       "      <td>0.016850</td>\n",
       "      <td>0.220475</td>\n",
       "      <td>0.004081</td>\n",
       "      <td>0.111231</td>\n",
       "      <td>0.370282</td>\n",
       "      <td>0.079181</td>\n",
       "      <td>-0.018715</td>\n",
       "      <td>0.110054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_21</th>\n",
       "      <td>0.012886</td>\n",
       "      <td>-0.027026</td>\n",
       "      <td>0.014113</td>\n",
       "      <td>0.354925</td>\n",
       "      <td>2.329227e-01</td>\n",
       "      <td>0.003288</td>\n",
       "      <td>0.008046</td>\n",
       "      <td>-0.022908</td>\n",
       "      <td>-0.041095</td>\n",
       "      <td>-0.028409</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004702</td>\n",
       "      <td>-0.006180</td>\n",
       "      <td>-0.045562</td>\n",
       "      <td>-0.016862</td>\n",
       "      <td>-0.030401</td>\n",
       "      <td>0.105392</td>\n",
       "      <td>-0.033193</td>\n",
       "      <td>-0.019779</td>\n",
       "      <td>0.058008</td>\n",
       "      <td>-0.007677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_22</th>\n",
       "      <td>0.190144</td>\n",
       "      <td>0.063283</td>\n",
       "      <td>0.215106</td>\n",
       "      <td>0.251082</td>\n",
       "      <td>2.477378e-01</td>\n",
       "      <td>0.075161</td>\n",
       "      <td>0.038939</td>\n",
       "      <td>0.162620</td>\n",
       "      <td>0.029032</td>\n",
       "      <td>-0.062348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041519</td>\n",
       "      <td>0.044396</td>\n",
       "      <td>-0.018347</td>\n",
       "      <td>0.219974</td>\n",
       "      <td>-0.045439</td>\n",
       "      <td>0.244779</td>\n",
       "      <td>0.098595</td>\n",
       "      <td>0.104921</td>\n",
       "      <td>0.200593</td>\n",
       "      <td>0.113276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_23</th>\n",
       "      <td>0.102980</td>\n",
       "      <td>0.048686</td>\n",
       "      <td>0.162065</td>\n",
       "      <td>-0.002427</td>\n",
       "      <td>3.062225e-02</td>\n",
       "      <td>0.017281</td>\n",
       "      <td>0.043651</td>\n",
       "      <td>0.186462</td>\n",
       "      <td>0.012774</td>\n",
       "      <td>0.006940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011832</td>\n",
       "      <td>0.056994</td>\n",
       "      <td>0.121170</td>\n",
       "      <td>0.111837</td>\n",
       "      <td>-0.014039</td>\n",
       "      <td>0.059743</td>\n",
       "      <td>0.141869</td>\n",
       "      <td>0.010438</td>\n",
       "      <td>-0.031837</td>\n",
       "      <td>0.084945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_24</th>\n",
       "      <td>0.215890</td>\n",
       "      <td>0.067255</td>\n",
       "      <td>0.253684</td>\n",
       "      <td>-0.031596</td>\n",
       "      <td>3.727726e-03</td>\n",
       "      <td>0.075222</td>\n",
       "      <td>0.082124</td>\n",
       "      <td>0.244813</td>\n",
       "      <td>0.161848</td>\n",
       "      <td>0.073618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091092</td>\n",
       "      <td>-0.018990</td>\n",
       "      <td>0.015444</td>\n",
       "      <td>0.123298</td>\n",
       "      <td>-0.043479</td>\n",
       "      <td>0.023581</td>\n",
       "      <td>0.357270</td>\n",
       "      <td>0.090833</td>\n",
       "      <td>-0.024375</td>\n",
       "      <td>0.089200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_25</th>\n",
       "      <td>-0.276763</td>\n",
       "      <td>0.187237</td>\n",
       "      <td>-0.096366</td>\n",
       "      <td>-0.157459</td>\n",
       "      <td>-1.342306e-01</td>\n",
       "      <td>-0.003610</td>\n",
       "      <td>-0.023319</td>\n",
       "      <td>-0.048820</td>\n",
       "      <td>-0.036939</td>\n",
       "      <td>-0.025279</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018320</td>\n",
       "      <td>0.021119</td>\n",
       "      <td>0.263924</td>\n",
       "      <td>-0.011294</td>\n",
       "      <td>0.207974</td>\n",
       "      <td>-0.012866</td>\n",
       "      <td>-0.088187</td>\n",
       "      <td>-0.045759</td>\n",
       "      <td>0.030135</td>\n",
       "      <td>-0.015708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_26</th>\n",
       "      <td>0.133917</td>\n",
       "      <td>-0.022813</td>\n",
       "      <td>0.064856</td>\n",
       "      <td>0.268112</td>\n",
       "      <td>3.657567e-01</td>\n",
       "      <td>0.025116</td>\n",
       "      <td>0.004680</td>\n",
       "      <td>-0.008782</td>\n",
       "      <td>-0.041599</td>\n",
       "      <td>-0.066414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048907</td>\n",
       "      <td>-0.048889</td>\n",
       "      <td>-0.072464</td>\n",
       "      <td>0.015937</td>\n",
       "      <td>-0.078470</td>\n",
       "      <td>0.094521</td>\n",
       "      <td>-0.021565</td>\n",
       "      <td>-0.018447</td>\n",
       "      <td>0.199974</td>\n",
       "      <td>0.016709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_27</th>\n",
       "      <td>0.097531</td>\n",
       "      <td>-0.038826</td>\n",
       "      <td>0.037841</td>\n",
       "      <td>0.508370</td>\n",
       "      <td>3.086287e-01</td>\n",
       "      <td>0.002098</td>\n",
       "      <td>0.001943</td>\n",
       "      <td>-0.015429</td>\n",
       "      <td>-0.050272</td>\n",
       "      <td>-0.042531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002502</td>\n",
       "      <td>-0.046053</td>\n",
       "      <td>-0.082510</td>\n",
       "      <td>-0.028097</td>\n",
       "      <td>-0.070194</td>\n",
       "      <td>0.099536</td>\n",
       "      <td>-0.025263</td>\n",
       "      <td>-0.018778</td>\n",
       "      <td>0.023790</td>\n",
       "      <td>0.000318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_28</th>\n",
       "      <td>0.128876</td>\n",
       "      <td>-0.030257</td>\n",
       "      <td>0.072494</td>\n",
       "      <td>0.551398</td>\n",
       "      <td>4.864171e-01</td>\n",
       "      <td>0.047688</td>\n",
       "      <td>0.017132</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>-0.036668</td>\n",
       "      <td>-0.055545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029148</td>\n",
       "      <td>-0.039784</td>\n",
       "      <td>-0.080806</td>\n",
       "      <td>-0.002941</td>\n",
       "      <td>-0.074442</td>\n",
       "      <td>0.169794</td>\n",
       "      <td>-0.021330</td>\n",
       "      <td>-0.015242</td>\n",
       "      <td>0.122653</td>\n",
       "      <td>0.005275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_29</th>\n",
       "      <td>0.117175</td>\n",
       "      <td>0.069266</td>\n",
       "      <td>0.025689</td>\n",
       "      <td>-0.004141</td>\n",
       "      <td>1.427066e-02</td>\n",
       "      <td>0.065957</td>\n",
       "      <td>0.002389</td>\n",
       "      <td>0.046231</td>\n",
       "      <td>0.104985</td>\n",
       "      <td>-0.021328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007335</td>\n",
       "      <td>0.013104</td>\n",
       "      <td>-0.011960</td>\n",
       "      <td>0.038800</td>\n",
       "      <td>-0.032585</td>\n",
       "      <td>0.055398</td>\n",
       "      <td>-0.000185</td>\n",
       "      <td>0.040526</td>\n",
       "      <td>0.084445</td>\n",
       "      <td>0.008301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_64</th>\n",
       "      <td>-0.124186</td>\n",
       "      <td>-0.010499</td>\n",
       "      <td>-0.005354</td>\n",
       "      <td>-0.065105</td>\n",
       "      <td>-4.728813e-02</td>\n",
       "      <td>-0.021017</td>\n",
       "      <td>-0.002764</td>\n",
       "      <td>0.011165</td>\n",
       "      <td>0.003194</td>\n",
       "      <td>0.702951</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013347</td>\n",
       "      <td>0.009820</td>\n",
       "      <td>0.064883</td>\n",
       "      <td>0.075964</td>\n",
       "      <td>0.117596</td>\n",
       "      <td>-0.015218</td>\n",
       "      <td>-0.001930</td>\n",
       "      <td>-0.020846</td>\n",
       "      <td>-0.071886</td>\n",
       "      <td>0.023064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_65</th>\n",
       "      <td>0.064755</td>\n",
       "      <td>0.110041</td>\n",
       "      <td>0.078801</td>\n",
       "      <td>0.065492</td>\n",
       "      <td>6.228472e-02</td>\n",
       "      <td>0.228349</td>\n",
       "      <td>0.066867</td>\n",
       "      <td>0.202346</td>\n",
       "      <td>0.025544</td>\n",
       "      <td>-0.038163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.120772</td>\n",
       "      <td>0.066922</td>\n",
       "      <td>0.030362</td>\n",
       "      <td>0.050138</td>\n",
       "      <td>-0.011600</td>\n",
       "      <td>0.125884</td>\n",
       "      <td>0.029076</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.044286</td>\n",
       "      <td>0.015500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_66</th>\n",
       "      <td>0.174734</td>\n",
       "      <td>0.053010</td>\n",
       "      <td>0.175620</td>\n",
       "      <td>0.088017</td>\n",
       "      <td>1.296545e-01</td>\n",
       "      <td>0.048364</td>\n",
       "      <td>0.033285</td>\n",
       "      <td>0.122660</td>\n",
       "      <td>0.115175</td>\n",
       "      <td>-0.001778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033794</td>\n",
       "      <td>0.031291</td>\n",
       "      <td>0.088686</td>\n",
       "      <td>0.206406</td>\n",
       "      <td>-0.000679</td>\n",
       "      <td>0.205289</td>\n",
       "      <td>0.094925</td>\n",
       "      <td>0.098063</td>\n",
       "      <td>0.123694</td>\n",
       "      <td>0.067957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_67</th>\n",
       "      <td>0.186622</td>\n",
       "      <td>0.154301</td>\n",
       "      <td>0.068667</td>\n",
       "      <td>-0.110081</td>\n",
       "      <td>-8.045694e-02</td>\n",
       "      <td>0.061964</td>\n",
       "      <td>0.038289</td>\n",
       "      <td>0.148598</td>\n",
       "      <td>0.320949</td>\n",
       "      <td>0.176921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039005</td>\n",
       "      <td>-0.020048</td>\n",
       "      <td>0.081445</td>\n",
       "      <td>0.295803</td>\n",
       "      <td>-0.058706</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>0.089262</td>\n",
       "      <td>0.112052</td>\n",
       "      <td>-0.011247</td>\n",
       "      <td>0.129018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_68</th>\n",
       "      <td>0.182707</td>\n",
       "      <td>0.014674</td>\n",
       "      <td>-0.012802</td>\n",
       "      <td>-0.030992</td>\n",
       "      <td>-2.009191e-02</td>\n",
       "      <td>0.107405</td>\n",
       "      <td>0.021619</td>\n",
       "      <td>0.040309</td>\n",
       "      <td>0.075384</td>\n",
       "      <td>-0.012192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036198</td>\n",
       "      <td>-0.024620</td>\n",
       "      <td>-0.038904</td>\n",
       "      <td>0.001672</td>\n",
       "      <td>-0.059843</td>\n",
       "      <td>0.125150</td>\n",
       "      <td>-0.023839</td>\n",
       "      <td>0.022515</td>\n",
       "      <td>0.095970</td>\n",
       "      <td>-0.004602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_69</th>\n",
       "      <td>0.210949</td>\n",
       "      <td>0.007544</td>\n",
       "      <td>0.307406</td>\n",
       "      <td>-0.032748</td>\n",
       "      <td>-1.446082e-02</td>\n",
       "      <td>-0.003294</td>\n",
       "      <td>0.074836</td>\n",
       "      <td>0.131430</td>\n",
       "      <td>0.046258</td>\n",
       "      <td>-0.029335</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007221</td>\n",
       "      <td>-0.006954</td>\n",
       "      <td>-0.025538</td>\n",
       "      <td>0.027690</td>\n",
       "      <td>-0.022918</td>\n",
       "      <td>0.011806</td>\n",
       "      <td>0.549489</td>\n",
       "      <td>0.041206</td>\n",
       "      <td>-0.037961</td>\n",
       "      <td>0.032052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_70</th>\n",
       "      <td>0.043618</td>\n",
       "      <td>0.165442</td>\n",
       "      <td>0.112968</td>\n",
       "      <td>-0.018774</td>\n",
       "      <td>2.079779e-02</td>\n",
       "      <td>0.118510</td>\n",
       "      <td>0.052401</td>\n",
       "      <td>0.237907</td>\n",
       "      <td>0.023089</td>\n",
       "      <td>-0.056205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045636</td>\n",
       "      <td>0.361941</td>\n",
       "      <td>0.225792</td>\n",
       "      <td>0.212133</td>\n",
       "      <td>0.140850</td>\n",
       "      <td>0.163631</td>\n",
       "      <td>0.074178</td>\n",
       "      <td>0.030560</td>\n",
       "      <td>0.007310</td>\n",
       "      <td>0.093488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_71</th>\n",
       "      <td>0.129365</td>\n",
       "      <td>0.013712</td>\n",
       "      <td>-0.002336</td>\n",
       "      <td>-0.053020</td>\n",
       "      <td>-4.241268e-02</td>\n",
       "      <td>0.056428</td>\n",
       "      <td>0.011901</td>\n",
       "      <td>0.115813</td>\n",
       "      <td>0.081664</td>\n",
       "      <td>0.043286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011858</td>\n",
       "      <td>0.013894</td>\n",
       "      <td>-0.015410</td>\n",
       "      <td>0.060004</td>\n",
       "      <td>-0.048676</td>\n",
       "      <td>0.076348</td>\n",
       "      <td>-0.019694</td>\n",
       "      <td>0.050622</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>0.002001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_72</th>\n",
       "      <td>-0.199876</td>\n",
       "      <td>-0.029983</td>\n",
       "      <td>-0.023267</td>\n",
       "      <td>-0.045339</td>\n",
       "      <td>-2.979578e-02</td>\n",
       "      <td>0.005177</td>\n",
       "      <td>-0.011090</td>\n",
       "      <td>-0.014921</td>\n",
       "      <td>-0.029868</td>\n",
       "      <td>-0.058147</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011992</td>\n",
       "      <td>0.294384</td>\n",
       "      <td>0.008897</td>\n",
       "      <td>0.013536</td>\n",
       "      <td>0.004066</td>\n",
       "      <td>0.057040</td>\n",
       "      <td>-0.030673</td>\n",
       "      <td>-0.008936</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>-0.008233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_73</th>\n",
       "      <td>0.075773</td>\n",
       "      <td>0.140815</td>\n",
       "      <td>0.039192</td>\n",
       "      <td>-0.013972</td>\n",
       "      <td>-1.128547e-02</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.025023</td>\n",
       "      <td>0.022819</td>\n",
       "      <td>0.028999</td>\n",
       "      <td>0.022679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012885</td>\n",
       "      <td>-0.010675</td>\n",
       "      <td>-0.000841</td>\n",
       "      <td>-0.004759</td>\n",
       "      <td>-0.026363</td>\n",
       "      <td>-0.006704</td>\n",
       "      <td>0.070001</td>\n",
       "      <td>0.007193</td>\n",
       "      <td>-0.024017</td>\n",
       "      <td>-0.000163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_74</th>\n",
       "      <td>0.062870</td>\n",
       "      <td>0.051365</td>\n",
       "      <td>0.070724</td>\n",
       "      <td>0.041559</td>\n",
       "      <td>4.909735e-02</td>\n",
       "      <td>0.017265</td>\n",
       "      <td>0.043160</td>\n",
       "      <td>0.053059</td>\n",
       "      <td>-0.000431</td>\n",
       "      <td>0.007594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028021</td>\n",
       "      <td>-0.000453</td>\n",
       "      <td>-0.015945</td>\n",
       "      <td>0.003992</td>\n",
       "      <td>-0.025207</td>\n",
       "      <td>0.042104</td>\n",
       "      <td>0.055372</td>\n",
       "      <td>0.016941</td>\n",
       "      <td>0.004497</td>\n",
       "      <td>0.021967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_75</th>\n",
       "      <td>0.199319</td>\n",
       "      <td>0.011596</td>\n",
       "      <td>0.093689</td>\n",
       "      <td>-0.044724</td>\n",
       "      <td>-3.145389e-02</td>\n",
       "      <td>0.015279</td>\n",
       "      <td>0.006951</td>\n",
       "      <td>0.039865</td>\n",
       "      <td>0.031466</td>\n",
       "      <td>-0.027313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028478</td>\n",
       "      <td>-0.026329</td>\n",
       "      <td>-0.031401</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>-0.058630</td>\n",
       "      <td>-0.014925</td>\n",
       "      <td>0.160418</td>\n",
       "      <td>-0.002625</td>\n",
       "      <td>-0.037710</td>\n",
       "      <td>0.006208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_76</th>\n",
       "      <td>0.189897</td>\n",
       "      <td>0.153808</td>\n",
       "      <td>0.259360</td>\n",
       "      <td>-0.028670</td>\n",
       "      <td>-1.379188e-02</td>\n",
       "      <td>0.035570</td>\n",
       "      <td>0.073867</td>\n",
       "      <td>0.375114</td>\n",
       "      <td>0.081682</td>\n",
       "      <td>-0.027424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031277</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.010324</td>\n",
       "      <td>0.063411</td>\n",
       "      <td>-0.050417</td>\n",
       "      <td>0.023242</td>\n",
       "      <td>0.291884</td>\n",
       "      <td>0.175163</td>\n",
       "      <td>-0.050887</td>\n",
       "      <td>0.029426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_77</th>\n",
       "      <td>0.053024</td>\n",
       "      <td>0.123752</td>\n",
       "      <td>0.014911</td>\n",
       "      <td>-0.001584</td>\n",
       "      <td>1.531773e-02</td>\n",
       "      <td>0.030462</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>0.005769</td>\n",
       "      <td>0.027486</td>\n",
       "      <td>-0.020185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>0.005602</td>\n",
       "      <td>0.020294</td>\n",
       "      <td>0.019275</td>\n",
       "      <td>-0.007396</td>\n",
       "      <td>0.021591</td>\n",
       "      <td>-0.004988</td>\n",
       "      <td>0.026376</td>\n",
       "      <td>0.076551</td>\n",
       "      <td>0.001715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_78</th>\n",
       "      <td>0.104017</td>\n",
       "      <td>0.279202</td>\n",
       "      <td>0.094256</td>\n",
       "      <td>-0.021979</td>\n",
       "      <td>-1.449856e-02</td>\n",
       "      <td>0.070709</td>\n",
       "      <td>0.061250</td>\n",
       "      <td>0.567084</td>\n",
       "      <td>0.079623</td>\n",
       "      <td>-0.015922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035861</td>\n",
       "      <td>0.004071</td>\n",
       "      <td>-0.018797</td>\n",
       "      <td>0.063539</td>\n",
       "      <td>-0.030010</td>\n",
       "      <td>0.014639</td>\n",
       "      <td>0.043339</td>\n",
       "      <td>0.068450</td>\n",
       "      <td>-0.028596</td>\n",
       "      <td>0.016047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_79</th>\n",
       "      <td>0.162963</td>\n",
       "      <td>0.228912</td>\n",
       "      <td>0.033668</td>\n",
       "      <td>-0.020566</td>\n",
       "      <td>-1.083473e-02</td>\n",
       "      <td>0.055115</td>\n",
       "      <td>0.009942</td>\n",
       "      <td>0.066753</td>\n",
       "      <td>0.083714</td>\n",
       "      <td>-0.036116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002540</td>\n",
       "      <td>0.004663</td>\n",
       "      <td>0.095254</td>\n",
       "      <td>0.099579</td>\n",
       "      <td>-0.018615</td>\n",
       "      <td>0.073207</td>\n",
       "      <td>0.031099</td>\n",
       "      <td>0.021616</td>\n",
       "      <td>0.162033</td>\n",
       "      <td>0.029082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_80</th>\n",
       "      <td>0.141716</td>\n",
       "      <td>-0.013303</td>\n",
       "      <td>0.155768</td>\n",
       "      <td>0.442036</td>\n",
       "      <td>4.057725e-01</td>\n",
       "      <td>0.026223</td>\n",
       "      <td>0.017648</td>\n",
       "      <td>0.028860</td>\n",
       "      <td>-0.038382</td>\n",
       "      <td>-0.046721</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029430</td>\n",
       "      <td>-0.035876</td>\n",
       "      <td>-0.081888</td>\n",
       "      <td>-0.004588</td>\n",
       "      <td>-0.076250</td>\n",
       "      <td>0.350787</td>\n",
       "      <td>0.012623</td>\n",
       "      <td>-0.017815</td>\n",
       "      <td>0.063401</td>\n",
       "      <td>0.012651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_81</th>\n",
       "      <td>0.078358</td>\n",
       "      <td>0.032427</td>\n",
       "      <td>0.052101</td>\n",
       "      <td>0.013089</td>\n",
       "      <td>2.828377e-02</td>\n",
       "      <td>0.129333</td>\n",
       "      <td>0.044136</td>\n",
       "      <td>0.144308</td>\n",
       "      <td>0.035102</td>\n",
       "      <td>-0.005847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096658</td>\n",
       "      <td>0.054972</td>\n",
       "      <td>0.013808</td>\n",
       "      <td>0.084096</td>\n",
       "      <td>-0.017469</td>\n",
       "      <td>0.166234</td>\n",
       "      <td>0.009379</td>\n",
       "      <td>0.017243</td>\n",
       "      <td>0.018565</td>\n",
       "      <td>0.019378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_82</th>\n",
       "      <td>0.113915</td>\n",
       "      <td>-0.026085</td>\n",
       "      <td>0.119109</td>\n",
       "      <td>0.438458</td>\n",
       "      <td>4.365413e-01</td>\n",
       "      <td>0.057400</td>\n",
       "      <td>0.014907</td>\n",
       "      <td>0.022059</td>\n",
       "      <td>-0.034409</td>\n",
       "      <td>-0.039806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054646</td>\n",
       "      <td>-0.034368</td>\n",
       "      <td>-0.065189</td>\n",
       "      <td>-0.012153</td>\n",
       "      <td>-0.059553</td>\n",
       "      <td>0.266249</td>\n",
       "      <td>-0.001795</td>\n",
       "      <td>-0.014641</td>\n",
       "      <td>0.049661</td>\n",
       "      <td>0.005497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_83</th>\n",
       "      <td>0.161417</td>\n",
       "      <td>0.059165</td>\n",
       "      <td>0.371691</td>\n",
       "      <td>-0.019914</td>\n",
       "      <td>-1.051874e-03</td>\n",
       "      <td>0.008006</td>\n",
       "      <td>0.035145</td>\n",
       "      <td>0.282069</td>\n",
       "      <td>0.033479</td>\n",
       "      <td>-0.032875</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001047</td>\n",
       "      <td>-0.009157</td>\n",
       "      <td>-0.029711</td>\n",
       "      <td>0.072006</td>\n",
       "      <td>-0.052930</td>\n",
       "      <td>0.035181</td>\n",
       "      <td>0.243942</td>\n",
       "      <td>0.095801</td>\n",
       "      <td>-0.018325</td>\n",
       "      <td>0.054188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_84</th>\n",
       "      <td>-0.008192</td>\n",
       "      <td>0.049634</td>\n",
       "      <td>0.009845</td>\n",
       "      <td>0.011159</td>\n",
       "      <td>5.684499e-03</td>\n",
       "      <td>0.467329</td>\n",
       "      <td>0.177777</td>\n",
       "      <td>0.062634</td>\n",
       "      <td>0.005064</td>\n",
       "      <td>-0.013569</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.010210</td>\n",
       "      <td>-0.003459</td>\n",
       "      <td>0.013631</td>\n",
       "      <td>-0.017903</td>\n",
       "      <td>0.103643</td>\n",
       "      <td>-0.006013</td>\n",
       "      <td>-0.003444</td>\n",
       "      <td>0.048431</td>\n",
       "      <td>0.003723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_85</th>\n",
       "      <td>-0.084507</td>\n",
       "      <td>-0.008739</td>\n",
       "      <td>-0.006764</td>\n",
       "      <td>-0.048626</td>\n",
       "      <td>-3.315343e-02</td>\n",
       "      <td>0.034062</td>\n",
       "      <td>0.004290</td>\n",
       "      <td>0.037874</td>\n",
       "      <td>-0.003416</td>\n",
       "      <td>-0.031462</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010210</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.109643</td>\n",
       "      <td>0.049250</td>\n",
       "      <td>0.027886</td>\n",
       "      <td>0.053582</td>\n",
       "      <td>-0.003931</td>\n",
       "      <td>-0.023091</td>\n",
       "      <td>-0.043484</td>\n",
       "      <td>0.023390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_86</th>\n",
       "      <td>-0.096484</td>\n",
       "      <td>0.107947</td>\n",
       "      <td>-0.039090</td>\n",
       "      <td>-0.096093</td>\n",
       "      <td>-7.102916e-02</td>\n",
       "      <td>0.013879</td>\n",
       "      <td>0.010455</td>\n",
       "      <td>-0.009169</td>\n",
       "      <td>-0.029395</td>\n",
       "      <td>-0.019144</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003459</td>\n",
       "      <td>0.109643</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.073685</td>\n",
       "      <td>0.426972</td>\n",
       "      <td>-0.011822</td>\n",
       "      <td>-0.019803</td>\n",
       "      <td>-0.024005</td>\n",
       "      <td>-0.049393</td>\n",
       "      <td>0.029035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_87</th>\n",
       "      <td>0.097087</td>\n",
       "      <td>0.089374</td>\n",
       "      <td>0.047451</td>\n",
       "      <td>-0.009838</td>\n",
       "      <td>5.054728e-03</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>0.015256</td>\n",
       "      <td>0.089574</td>\n",
       "      <td>0.059929</td>\n",
       "      <td>-0.016925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013631</td>\n",
       "      <td>0.049250</td>\n",
       "      <td>0.073685</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.023053</td>\n",
       "      <td>0.066008</td>\n",
       "      <td>0.014696</td>\n",
       "      <td>0.028850</td>\n",
       "      <td>0.001424</td>\n",
       "      <td>0.499990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_88</th>\n",
       "      <td>-0.215878</td>\n",
       "      <td>0.020830</td>\n",
       "      <td>-0.047035</td>\n",
       "      <td>-0.082336</td>\n",
       "      <td>-6.748367e-02</td>\n",
       "      <td>-0.019201</td>\n",
       "      <td>-0.015437</td>\n",
       "      <td>-0.033646</td>\n",
       "      <td>-0.050931</td>\n",
       "      <td>0.001160</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017903</td>\n",
       "      <td>0.027886</td>\n",
       "      <td>0.426972</td>\n",
       "      <td>0.023053</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.022552</td>\n",
       "      <td>-0.031679</td>\n",
       "      <td>-0.033653</td>\n",
       "      <td>-0.070120</td>\n",
       "      <td>-0.008631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_89</th>\n",
       "      <td>0.111519</td>\n",
       "      <td>0.096851</td>\n",
       "      <td>0.105527</td>\n",
       "      <td>0.174781</td>\n",
       "      <td>1.837145e-01</td>\n",
       "      <td>0.119951</td>\n",
       "      <td>0.035042</td>\n",
       "      <td>0.063511</td>\n",
       "      <td>0.007974</td>\n",
       "      <td>-0.019147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103643</td>\n",
       "      <td>0.053582</td>\n",
       "      <td>-0.011822</td>\n",
       "      <td>0.066008</td>\n",
       "      <td>-0.022552</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027764</td>\n",
       "      <td>0.015917</td>\n",
       "      <td>0.129622</td>\n",
       "      <td>0.030650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_90</th>\n",
       "      <td>0.188895</td>\n",
       "      <td>0.010310</td>\n",
       "      <td>0.515022</td>\n",
       "      <td>-0.015068</td>\n",
       "      <td>9.454061e-03</td>\n",
       "      <td>0.004842</td>\n",
       "      <td>0.054034</td>\n",
       "      <td>0.129578</td>\n",
       "      <td>0.026807</td>\n",
       "      <td>-0.020698</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006013</td>\n",
       "      <td>-0.003931</td>\n",
       "      <td>-0.019803</td>\n",
       "      <td>0.014696</td>\n",
       "      <td>-0.031679</td>\n",
       "      <td>0.027764</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014812</td>\n",
       "      <td>-0.035311</td>\n",
       "      <td>0.039864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_91</th>\n",
       "      <td>0.139078</td>\n",
       "      <td>0.037264</td>\n",
       "      <td>0.026383</td>\n",
       "      <td>-0.012417</td>\n",
       "      <td>-1.031241e-02</td>\n",
       "      <td>0.012012</td>\n",
       "      <td>0.012465</td>\n",
       "      <td>0.068506</td>\n",
       "      <td>0.095990</td>\n",
       "      <td>-0.014742</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003444</td>\n",
       "      <td>-0.023091</td>\n",
       "      <td>-0.024005</td>\n",
       "      <td>0.028850</td>\n",
       "      <td>-0.033653</td>\n",
       "      <td>0.015917</td>\n",
       "      <td>0.014812</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.104226</td>\n",
       "      <td>-0.000045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_92</th>\n",
       "      <td>0.131737</td>\n",
       "      <td>0.054777</td>\n",
       "      <td>-0.008219</td>\n",
       "      <td>0.066921</td>\n",
       "      <td>8.763105e-02</td>\n",
       "      <td>0.065331</td>\n",
       "      <td>0.015479</td>\n",
       "      <td>-0.032261</td>\n",
       "      <td>0.013608</td>\n",
       "      <td>-0.069707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048431</td>\n",
       "      <td>-0.043484</td>\n",
       "      <td>-0.049393</td>\n",
       "      <td>0.001424</td>\n",
       "      <td>-0.070120</td>\n",
       "      <td>0.129622</td>\n",
       "      <td>-0.035311</td>\n",
       "      <td>0.104226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.003653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_93</th>\n",
       "      <td>0.047944</td>\n",
       "      <td>0.081783</td>\n",
       "      <td>0.054593</td>\n",
       "      <td>0.006814</td>\n",
       "      <td>1.574563e-02</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.008521</td>\n",
       "      <td>0.034912</td>\n",
       "      <td>0.005131</td>\n",
       "      <td>-0.006038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>0.023390</td>\n",
       "      <td>0.029035</td>\n",
       "      <td>0.499990</td>\n",
       "      <td>-0.008631</td>\n",
       "      <td>0.030650</td>\n",
       "      <td>0.039864</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.003653</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94 rows × 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id    feat_1    feat_2    feat_3        feat_4    feat_5  \\\n",
       "id       1.000000  0.070691  0.185463  0.138980  1.460200e-01  0.030707   \n",
       "feat_1   0.070691  1.000000  0.031332 -0.027807 -2.752941e-02  0.042973   \n",
       "feat_2   0.185463  0.031332  1.000000  0.082573  1.349870e-01  0.020926   \n",
       "feat_3   0.138980 -0.027807  0.082573  1.000000  5.835232e-01  0.010880   \n",
       "feat_4   0.146020 -0.027529  0.134987  0.583523  1.000000e+00  0.017290   \n",
       "feat_5   0.030707  0.042973  0.020926  0.010880  1.729026e-02  1.000000   \n",
       "feat_6   0.033410  0.043603  0.041343  0.004288  1.405895e-02  0.145355   \n",
       "feat_7   0.136501  0.298952  0.222386  0.001294  1.448981e-02  0.075047   \n",
       "feat_8   0.197418  0.056321  0.019815 -0.053462 -4.618407e-02  0.035861   \n",
       "feat_9  -0.156082 -0.032285 -0.025630 -0.063551 -4.624977e-02 -0.024708   \n",
       "feat_10  0.096127  0.097776  0.051925  0.036944  5.951396e-02  0.091324   \n",
       "feat_11  0.179164 -0.042928  0.118534  0.596243  3.894092e-01  0.004882   \n",
       "feat_12  0.079170  0.056934  0.090153  0.050037  5.743356e-02  0.036668   \n",
       "feat_13  0.184629  0.139254  0.157467  0.013870  2.897317e-02  0.059081   \n",
       "feat_14 -0.346308  0.063517 -0.070057 -0.111105 -9.921490e-02 -0.037607   \n",
       "feat_15 -0.245562 -0.045738 -0.048798 -0.065285 -5.122155e-02 -0.007000   \n",
       "feat_16 -0.004622  0.027086  0.108046  0.221426  2.110780e-01  0.062877   \n",
       "feat_17  0.122884  0.053004  0.074902 -0.023093 -7.553867e-03  0.062197   \n",
       "feat_18  0.189280  0.084856  0.242716  0.115655  2.148952e-01  0.052186   \n",
       "feat_19  0.127893  0.002302  0.176655 -0.012228 -3.519107e-07 -0.008556   \n",
       "feat_20  0.213630  0.070511  0.449160 -0.011069  4.465657e-02  0.046200   \n",
       "feat_21  0.012886 -0.027026  0.014113  0.354925  2.329227e-01  0.003288   \n",
       "feat_22  0.190144  0.063283  0.215106  0.251082  2.477378e-01  0.075161   \n",
       "feat_23  0.102980  0.048686  0.162065 -0.002427  3.062225e-02  0.017281   \n",
       "feat_24  0.215890  0.067255  0.253684 -0.031596  3.727726e-03  0.075222   \n",
       "feat_25 -0.276763  0.187237 -0.096366 -0.157459 -1.342306e-01 -0.003610   \n",
       "feat_26  0.133917 -0.022813  0.064856  0.268112  3.657567e-01  0.025116   \n",
       "feat_27  0.097531 -0.038826  0.037841  0.508370  3.086287e-01  0.002098   \n",
       "feat_28  0.128876 -0.030257  0.072494  0.551398  4.864171e-01  0.047688   \n",
       "feat_29  0.117175  0.069266  0.025689 -0.004141  1.427066e-02  0.065957   \n",
       "...           ...       ...       ...       ...           ...       ...   \n",
       "feat_64 -0.124186 -0.010499 -0.005354 -0.065105 -4.728813e-02 -0.021017   \n",
       "feat_65  0.064755  0.110041  0.078801  0.065492  6.228472e-02  0.228349   \n",
       "feat_66  0.174734  0.053010  0.175620  0.088017  1.296545e-01  0.048364   \n",
       "feat_67  0.186622  0.154301  0.068667 -0.110081 -8.045694e-02  0.061964   \n",
       "feat_68  0.182707  0.014674 -0.012802 -0.030992 -2.009191e-02  0.107405   \n",
       "feat_69  0.210949  0.007544  0.307406 -0.032748 -1.446082e-02 -0.003294   \n",
       "feat_70  0.043618  0.165442  0.112968 -0.018774  2.079779e-02  0.118510   \n",
       "feat_71  0.129365  0.013712 -0.002336 -0.053020 -4.241268e-02  0.056428   \n",
       "feat_72 -0.199876 -0.029983 -0.023267 -0.045339 -2.979578e-02  0.005177   \n",
       "feat_73  0.075773  0.140815  0.039192 -0.013972 -1.128547e-02  0.001609   \n",
       "feat_74  0.062870  0.051365  0.070724  0.041559  4.909735e-02  0.017265   \n",
       "feat_75  0.199319  0.011596  0.093689 -0.044724 -3.145389e-02  0.015279   \n",
       "feat_76  0.189897  0.153808  0.259360 -0.028670 -1.379188e-02  0.035570   \n",
       "feat_77  0.053024  0.123752  0.014911 -0.001584  1.531773e-02  0.030462   \n",
       "feat_78  0.104017  0.279202  0.094256 -0.021979 -1.449856e-02  0.070709   \n",
       "feat_79  0.162963  0.228912  0.033668 -0.020566 -1.083473e-02  0.055115   \n",
       "feat_80  0.141716 -0.013303  0.155768  0.442036  4.057725e-01  0.026223   \n",
       "feat_81  0.078358  0.032427  0.052101  0.013089  2.828377e-02  0.129333   \n",
       "feat_82  0.113915 -0.026085  0.119109  0.438458  4.365413e-01  0.057400   \n",
       "feat_83  0.161417  0.059165  0.371691 -0.019914 -1.051874e-03  0.008006   \n",
       "feat_84 -0.008192  0.049634  0.009845  0.011159  5.684499e-03  0.467329   \n",
       "feat_85 -0.084507 -0.008739 -0.006764 -0.048626 -3.315343e-02  0.034062   \n",
       "feat_86 -0.096484  0.107947 -0.039090 -0.096093 -7.102916e-02  0.013879   \n",
       "feat_87  0.097087  0.089374  0.047451 -0.009838  5.054728e-03  0.013999   \n",
       "feat_88 -0.215878  0.020830 -0.047035 -0.082336 -6.748367e-02 -0.019201   \n",
       "feat_89  0.111519  0.096851  0.105527  0.174781  1.837145e-01  0.119951   \n",
       "feat_90  0.188895  0.010310  0.515022 -0.015068  9.454061e-03  0.004842   \n",
       "feat_91  0.139078  0.037264  0.026383 -0.012417 -1.031241e-02  0.012012   \n",
       "feat_92  0.131737  0.054777 -0.008219  0.066921  8.763105e-02  0.065331   \n",
       "feat_93  0.047944  0.081783  0.054593  0.006814  1.574563e-02  0.002038   \n",
       "\n",
       "           feat_6    feat_7    feat_8    feat_9    ...      feat_84   feat_85  \\\n",
       "id       0.033410  0.136501  0.197418 -0.156082    ...    -0.008192 -0.084507   \n",
       "feat_1   0.043603  0.298952  0.056321 -0.032285    ...     0.049634 -0.008739   \n",
       "feat_2   0.041343  0.222386  0.019815 -0.025630    ...     0.009845 -0.006764   \n",
       "feat_3   0.004288  0.001294 -0.053462 -0.063551    ...     0.011159 -0.048626   \n",
       "feat_4   0.014059  0.014490 -0.046184 -0.046250    ...     0.005684 -0.033153   \n",
       "feat_5   0.145355  0.075047  0.035861 -0.024708    ...     0.467329  0.034062   \n",
       "feat_6   1.000000  0.088014  0.012867 -0.009373    ...     0.177777  0.004290   \n",
       "feat_7   0.088014  1.000000  0.038121 -0.027146    ...     0.062634  0.037874   \n",
       "feat_8   0.012867  0.038121  1.000000 -0.039281    ...     0.005064 -0.003416   \n",
       "feat_9  -0.009373 -0.027146 -0.039281  1.000000    ...    -0.013569 -0.031462   \n",
       "feat_10  0.041940  0.194258 -0.000023 -0.024323    ...     0.017939  0.086758   \n",
       "feat_11  0.014504  0.012418 -0.065923 -0.075820    ...     0.017724 -0.074293   \n",
       "feat_12  0.028588  0.056230  0.091424 -0.021885    ...     0.009807  0.019283   \n",
       "feat_13  0.036293  0.199142  0.095365 -0.040164    ...     0.023221  0.002594   \n",
       "feat_14 -0.027350 -0.044671 -0.061799 -0.110188    ...    -0.027058 -0.021455   \n",
       "feat_15 -0.018328 -0.035721 -0.056960  0.009858    ...    -0.009311  0.246847   \n",
       "feat_16  0.021934  0.043957 -0.004659 -0.082664    ...     0.035211  0.110850   \n",
       "feat_17  0.015488  0.127245  0.173912 -0.028709    ...     0.000334  0.015559   \n",
       "feat_18  0.048710  0.098972  0.087777 -0.043642    ...     0.028752 -0.001555   \n",
       "feat_19  0.038493  0.058071  0.019387 -0.000167    ...    -0.002847 -0.008292   \n",
       "feat_20  0.057813  0.364972  0.062595 -0.023397    ...     0.001723  0.084570   \n",
       "feat_21  0.008046 -0.022908 -0.041095 -0.028409    ...    -0.004702 -0.006180   \n",
       "feat_22  0.038939  0.162620  0.029032 -0.062348    ...     0.041519  0.044396   \n",
       "feat_23  0.043651  0.186462  0.012774  0.006940    ...     0.011832  0.056994   \n",
       "feat_24  0.082124  0.244813  0.161848  0.073618    ...     0.091092 -0.018990   \n",
       "feat_25 -0.023319 -0.048820 -0.036939 -0.025279    ...    -0.018320  0.021119   \n",
       "feat_26  0.004680 -0.008782 -0.041599 -0.066414    ...     0.048907 -0.048889   \n",
       "feat_27  0.001943 -0.015429 -0.050272 -0.042531    ...     0.002502 -0.046053   \n",
       "feat_28  0.017132  0.000998 -0.036668 -0.055545    ...     0.029148 -0.039784   \n",
       "feat_29  0.002389  0.046231  0.104985 -0.021328    ...     0.007335  0.013104   \n",
       "...           ...       ...       ...       ...    ...          ...       ...   \n",
       "feat_64 -0.002764  0.011165  0.003194  0.702951    ...    -0.013347  0.009820   \n",
       "feat_65  0.066867  0.202346  0.025544 -0.038163    ...     0.120772  0.066922   \n",
       "feat_66  0.033285  0.122660  0.115175 -0.001778    ...     0.033794  0.031291   \n",
       "feat_67  0.038289  0.148598  0.320949  0.176921    ...     0.039005 -0.020048   \n",
       "feat_68  0.021619  0.040309  0.075384 -0.012192    ...     0.036198 -0.024620   \n",
       "feat_69  0.074836  0.131430  0.046258 -0.029335    ...    -0.007221 -0.006954   \n",
       "feat_70  0.052401  0.237907  0.023089 -0.056205    ...     0.045636  0.361941   \n",
       "feat_71  0.011901  0.115813  0.081664  0.043286    ...     0.011858  0.013894   \n",
       "feat_72 -0.011090 -0.014921 -0.029868 -0.058147    ...    -0.011992  0.294384   \n",
       "feat_73  0.025023  0.022819  0.028999  0.022679    ...     0.012885 -0.010675   \n",
       "feat_74  0.043160  0.053059 -0.000431  0.007594    ...     0.028021 -0.000453   \n",
       "feat_75  0.006951  0.039865  0.031466 -0.027313    ...     0.028478 -0.026329   \n",
       "feat_76  0.073867  0.375114  0.081682 -0.027424    ...     0.031277  0.000682   \n",
       "feat_77  0.006501  0.005769  0.027486 -0.020185    ...     0.001243  0.005602   \n",
       "feat_78  0.061250  0.567084  0.079623 -0.015922    ...     0.035861  0.004071   \n",
       "feat_79  0.009942  0.066753  0.083714 -0.036116    ...     0.002540  0.004663   \n",
       "feat_80  0.017648  0.028860 -0.038382 -0.046721    ...     0.029430 -0.035876   \n",
       "feat_81  0.044136  0.144308  0.035102 -0.005847    ...     0.096658  0.054972   \n",
       "feat_82  0.014907  0.022059 -0.034409 -0.039806    ...     0.054646 -0.034368   \n",
       "feat_83  0.035145  0.282069  0.033479 -0.032875    ...    -0.001047 -0.009157   \n",
       "feat_84  0.177777  0.062634  0.005064 -0.013569    ...     1.000000 -0.010210   \n",
       "feat_85  0.004290  0.037874 -0.003416 -0.031462    ...    -0.010210  1.000000   \n",
       "feat_86  0.010455 -0.009169 -0.029395 -0.019144    ...    -0.003459  0.109643   \n",
       "feat_87  0.015256  0.089574  0.059929 -0.016925    ...     0.013631  0.049250   \n",
       "feat_88 -0.015437 -0.033646 -0.050931  0.001160    ...    -0.017903  0.027886   \n",
       "feat_89  0.035042  0.063511  0.007974 -0.019147    ...     0.103643  0.053582   \n",
       "feat_90  0.054034  0.129578  0.026807 -0.020698    ...    -0.006013 -0.003931   \n",
       "feat_91  0.012465  0.068506  0.095990 -0.014742    ...    -0.003444 -0.023091   \n",
       "feat_92  0.015479 -0.032261  0.013608 -0.069707    ...     0.048431 -0.043484   \n",
       "feat_93  0.008521  0.034912  0.005131 -0.006038    ...     0.003723  0.023390   \n",
       "\n",
       "          feat_86   feat_87   feat_88   feat_89   feat_90   feat_91   feat_92  \\\n",
       "id      -0.096484  0.097087 -0.215878  0.111519  0.188895  0.139078  0.131737   \n",
       "feat_1   0.107947  0.089374  0.020830  0.096851  0.010310  0.037264  0.054777   \n",
       "feat_2  -0.039090  0.047451 -0.047035  0.105527  0.515022  0.026383 -0.008219   \n",
       "feat_3  -0.096093 -0.009838 -0.082336  0.174781 -0.015068 -0.012417  0.066921   \n",
       "feat_4  -0.071029  0.005055 -0.067484  0.183715  0.009454 -0.010312  0.087631   \n",
       "feat_5   0.013879  0.013999 -0.019201  0.119951  0.004842  0.012012  0.065331   \n",
       "feat_6   0.010455  0.015256 -0.015437  0.035042  0.054034  0.012465  0.015479   \n",
       "feat_7  -0.009169  0.089574 -0.033646  0.063511  0.129578  0.068506 -0.032261   \n",
       "feat_8  -0.029395  0.059929 -0.050931  0.007974  0.026807  0.095990  0.013608   \n",
       "feat_9  -0.019144 -0.016925  0.001160 -0.019147 -0.020698 -0.014742 -0.069707   \n",
       "feat_10  0.159447  0.077421  0.054635  0.061498  0.049908  0.024025 -0.006869   \n",
       "feat_11 -0.123339 -0.032969 -0.114491  0.137374  0.045074 -0.029511  0.013179   \n",
       "feat_12 -0.007214  0.016089 -0.024324  0.082220  0.062721  0.063965  0.063922   \n",
       "feat_13  0.004850  0.093870 -0.036259  0.062990  0.107722  0.044338  0.071953   \n",
       "feat_14  0.145787 -0.020229  0.323089 -0.038881 -0.060240 -0.038444 -0.040133   \n",
       "feat_15 -0.002529 -0.023191  0.010840  0.029547 -0.046616 -0.034402 -0.018206   \n",
       "feat_16  0.003610  0.077770 -0.007257  0.248364  0.016863  0.048494  0.210499   \n",
       "feat_17  0.049102  0.214221 -0.034139  0.035390  0.045218  0.088508 -0.006538   \n",
       "feat_18 -0.029295  0.126886 -0.035981  0.247462  0.094336  0.037275  0.126640   \n",
       "feat_19 -0.014560  0.000412 -0.018485  0.011116  0.450925  0.004085 -0.027662   \n",
       "feat_20  0.016850  0.220475  0.004081  0.111231  0.370282  0.079181 -0.018715   \n",
       "feat_21 -0.045562 -0.016862 -0.030401  0.105392 -0.033193 -0.019779  0.058008   \n",
       "feat_22 -0.018347  0.219974 -0.045439  0.244779  0.098595  0.104921  0.200593   \n",
       "feat_23  0.121170  0.111837 -0.014039  0.059743  0.141869  0.010438 -0.031837   \n",
       "feat_24  0.015444  0.123298 -0.043479  0.023581  0.357270  0.090833 -0.024375   \n",
       "feat_25  0.263924 -0.011294  0.207974 -0.012866 -0.088187 -0.045759  0.030135   \n",
       "feat_26 -0.072464  0.015937 -0.078470  0.094521 -0.021565 -0.018447  0.199974   \n",
       "feat_27 -0.082510 -0.028097 -0.070194  0.099536 -0.025263 -0.018778  0.023790   \n",
       "feat_28 -0.080806 -0.002941 -0.074442  0.169794 -0.021330 -0.015242  0.122653   \n",
       "feat_29 -0.011960  0.038800 -0.032585  0.055398 -0.000185  0.040526  0.084445   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "feat_64  0.064883  0.075964  0.117596 -0.015218 -0.001930 -0.020846 -0.071886   \n",
       "feat_65  0.030362  0.050138 -0.011600  0.125884  0.029076  0.001188  0.044286   \n",
       "feat_66  0.088686  0.206406 -0.000679  0.205289  0.094925  0.098063  0.123694   \n",
       "feat_67  0.081445  0.295803 -0.058706  0.005220  0.089262  0.112052 -0.011247   \n",
       "feat_68 -0.038904  0.001672 -0.059843  0.125150 -0.023839  0.022515  0.095970   \n",
       "feat_69 -0.025538  0.027690 -0.022918  0.011806  0.549489  0.041206 -0.037961   \n",
       "feat_70  0.225792  0.212133  0.140850  0.163631  0.074178  0.030560  0.007310   \n",
       "feat_71 -0.015410  0.060004 -0.048676  0.076348 -0.019694  0.050622  0.000368   \n",
       "feat_72  0.008897  0.013536  0.004066  0.057040 -0.030673 -0.008936  0.005300   \n",
       "feat_73 -0.000841 -0.004759 -0.026363 -0.006704  0.070001  0.007193 -0.024017   \n",
       "feat_74 -0.015945  0.003992 -0.025207  0.042104  0.055372  0.016941  0.004497   \n",
       "feat_75 -0.031401  0.001201 -0.058630 -0.014925  0.160418 -0.002625 -0.037710   \n",
       "feat_76  0.010324  0.063411 -0.050417  0.023242  0.291884  0.175163 -0.050887   \n",
       "feat_77  0.020294  0.019275 -0.007396  0.021591 -0.004988  0.026376  0.076551   \n",
       "feat_78 -0.018797  0.063539 -0.030010  0.014639  0.043339  0.068450 -0.028596   \n",
       "feat_79  0.095254  0.099579 -0.018615  0.073207  0.031099  0.021616  0.162033   \n",
       "feat_80 -0.081888 -0.004588 -0.076250  0.350787  0.012623 -0.017815  0.063401   \n",
       "feat_81  0.013808  0.084096 -0.017469  0.166234  0.009379  0.017243  0.018565   \n",
       "feat_82 -0.065189 -0.012153 -0.059553  0.266249 -0.001795 -0.014641  0.049661   \n",
       "feat_83 -0.029711  0.072006 -0.052930  0.035181  0.243942  0.095801 -0.018325   \n",
       "feat_84 -0.003459  0.013631 -0.017903  0.103643 -0.006013 -0.003444  0.048431   \n",
       "feat_85  0.109643  0.049250  0.027886  0.053582 -0.003931 -0.023091 -0.043484   \n",
       "feat_86  1.000000  0.073685  0.426972 -0.011822 -0.019803 -0.024005 -0.049393   \n",
       "feat_87  0.073685  1.000000  0.023053  0.066008  0.014696  0.028850  0.001424   \n",
       "feat_88  0.426972  0.023053  1.000000 -0.022552 -0.031679 -0.033653 -0.070120   \n",
       "feat_89 -0.011822  0.066008 -0.022552  1.000000  0.027764  0.015917  0.129622   \n",
       "feat_90 -0.019803  0.014696 -0.031679  0.027764  1.000000  0.014812 -0.035311   \n",
       "feat_91 -0.024005  0.028850 -0.033653  0.015917  0.014812  1.000000  0.104226   \n",
       "feat_92 -0.049393  0.001424 -0.070120  0.129622 -0.035311  0.104226  1.000000   \n",
       "feat_93  0.029035  0.499990 -0.008631  0.030650  0.039864 -0.000045 -0.003653   \n",
       "\n",
       "          feat_93  \n",
       "id       0.047944  \n",
       "feat_1   0.081783  \n",
       "feat_2   0.054593  \n",
       "feat_3   0.006814  \n",
       "feat_4   0.015746  \n",
       "feat_5   0.002038  \n",
       "feat_6   0.008521  \n",
       "feat_7   0.034912  \n",
       "feat_8   0.005131  \n",
       "feat_9  -0.006038  \n",
       "feat_10  0.041316  \n",
       "feat_11  0.003326  \n",
       "feat_12  0.012722  \n",
       "feat_13  0.038989  \n",
       "feat_14 -0.018127  \n",
       "feat_15 -0.020369  \n",
       "feat_16  0.031467  \n",
       "feat_17  0.056695  \n",
       "feat_18  0.058100  \n",
       "feat_19  0.014243  \n",
       "feat_20  0.110054  \n",
       "feat_21 -0.007677  \n",
       "feat_22  0.113276  \n",
       "feat_23  0.084945  \n",
       "feat_24  0.089200  \n",
       "feat_25 -0.015708  \n",
       "feat_26  0.016709  \n",
       "feat_27  0.000318  \n",
       "feat_28  0.005275  \n",
       "feat_29  0.008301  \n",
       "...           ...  \n",
       "feat_64  0.023064  \n",
       "feat_65  0.015500  \n",
       "feat_66  0.067957  \n",
       "feat_67  0.129018  \n",
       "feat_68 -0.004602  \n",
       "feat_69  0.032052  \n",
       "feat_70  0.093488  \n",
       "feat_71  0.002001  \n",
       "feat_72 -0.008233  \n",
       "feat_73 -0.000163  \n",
       "feat_74  0.021967  \n",
       "feat_75  0.006208  \n",
       "feat_76  0.029426  \n",
       "feat_77  0.001715  \n",
       "feat_78  0.016047  \n",
       "feat_79  0.029082  \n",
       "feat_80  0.012651  \n",
       "feat_81  0.019378  \n",
       "feat_82  0.005497  \n",
       "feat_83  0.054188  \n",
       "feat_84  0.003723  \n",
       "feat_85  0.023390  \n",
       "feat_86  0.029035  \n",
       "feat_87  0.499990  \n",
       "feat_88 -0.008631  \n",
       "feat_89  0.030650  \n",
       "feat_90  0.039864  \n",
       "feat_91 -0.000045  \n",
       "feat_92 -0.003653  \n",
       "feat_93  1.000000  \n",
       "\n",
       "[94 rows x 94 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OFUrwoRwf5XJ"
   },
   "outputs": [],
   "source": [
    "target_dict = {'Class_1': 1,\n",
    "               'Class_2': 2,\n",
    "               'Class_3': 3,\n",
    "               'Class_4': 4,\n",
    "               'Class_5': 5,\n",
    "               'Class_6': 6,\n",
    "               'Class_7': 7,\n",
    "               'Class_8': 8,\n",
    "               'Class_9': 9}\n",
    "dataset_train['target']= dataset_train['target'].apply(lambda x: target_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "colab_type": "code",
    "id": "OfP-6f29f-a7",
    "outputId": "668fbe9b-4b68-49ba-acf3-ae681fa378b7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  feat_9  \\\n",
       "0   1       1       0       0       0       0       0       0       0       0   \n",
       "1   2       0       0       0       0       0       0       0       1       0   \n",
       "2   3       0       0       0       0       0       0       0       1       0   \n",
       "3   4       1       0       0       1       6       1       5       0       0   \n",
       "4   5       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "    ...    feat_85  feat_86  feat_87  feat_88  feat_89  feat_90  feat_91  \\\n",
       "0   ...          1        0        0        0        0        0        0   \n",
       "1   ...          0        0        0        0        0        0        0   \n",
       "2   ...          0        0        0        0        0        0        0   \n",
       "3   ...          0        1        2        0        0        0        0   \n",
       "4   ...          1        0        0        0        0        1        0   \n",
       "\n",
       "   feat_92  feat_93  target  \n",
       "0        0        0       1  \n",
       "1        0        0       1  \n",
       "2        0        0       1  \n",
       "3        0        0       1  \n",
       "4        0        0       1  \n",
       "\n",
       "[5 rows x 95 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "utmzV6iPgC8a"
   },
   "source": [
    "We can see that the target values changed into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "id": "TccHibQpgAtT",
    "outputId": "5e8944ce-4a57-481d-f2dd-651b2fdc4c7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,     1,     0, ...,     0,     0,     0],\n",
       "       [    2,     0,     0, ...,     0,     0,     0],\n",
       "       [    3,     0,     0, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [61876,     0,     0, ...,     0,     0,     0],\n",
       "       [61877,     1,     0, ...,     3,    10,     0],\n",
       "       [61878,     0,     0, ...,     0,     2,     0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset_train.iloc[:, :-1].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "id": "PRdVKK3CgGXq",
    "outputId": "32c96d97-d10d-4b4d-e29b-90a888a4c6bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [9],\n",
       "       [9],\n",
       "       [9]])"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = dataset_train.iloc[:, -1:].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "xJQyy9FDgLVh",
    "outputId": "b18083e5-0f79-43e7-e651-24a7146c559f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61878, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ey5AhqZvgQ1V"
   },
   "outputs": [],
   "source": [
    "#splitting the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ZAe_dMggTU7"
   },
   "outputs": [],
   "source": [
    "X_test = dataset_test.iloc[:,:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "Smm9lmCngVgt",
    "outputId": "bd47514c-23b2-448d-e07a-228d16bd5fbe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#feature normalisation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_x = StandardScaler()\n",
    "sc_x = sc_x.fit(X_train)\n",
    "X_train = sc_x.transform(X_train)\n",
    "X_val = sc_x.transform(X_val)\n",
    "X_test = sc_x.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8teteyR-gYHI"
   },
   "outputs": [],
   "source": [
    "y_train = y_train.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "yqXQFPVUgdoz",
    "outputId": "5d2319dd-7d02-4a6b-b306-404e32e262b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46408,)"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v0XX4UvzgkRp"
   },
   "source": [
    "## **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CxawDjmTgfZj"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier_LR = LogisticRegression()\n",
    "classifier_LR.fit(X_train, y_train)\n",
    "y_pred_LR_val = classifier_LR.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "_uhlgmomgoo5",
    "outputId": "199a621b-d348-4ec0-8d46-564493076308"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.86"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy of Logistic Regression in training set\n",
    "accuracy_classifier_LR = round(classifier_LR.score(X_train,y_train)*100, 2)\n",
    "accuracy_classifier_LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nHDwfojchGhD"
   },
   "source": [
    "We can see that the training set accuracy is about around 89.9%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k7qO2Mlyg6OQ"
   },
   "source": [
    "#### **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "CYr_yry1g09q",
    "outputId": "bb8bceeb-0352-4700-e827-f7df5f250b0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8899806076276664\n"
     ]
    }
   ],
   "source": [
    "#1. Validation score\n",
    "from sklearn.metrics import accuracy_score\n",
    "val_acc_LR = accuracy_score(y_val, y_pred_LR_val)\n",
    "print(val_acc_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NCOBSEyfhP3y"
   },
   "source": [
    "We can see that the validation accuracy is about 88.9%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "WUPkeTFfg_cA",
    "outputId": "8175d2f6-085b-40ae-b5d6-c2cc963e94d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 357   96    0    0    0    7    0    0    0]\n",
      " [   2 3901   87    1    4    7    3    0    0]\n",
      " [   0  357 1509   32    2   22   24    1    0]\n",
      " [   0    7  352  290    6   45   10    0    0]\n",
      " [   0    0   18    0  642    4    0    1    0]\n",
      " [   0    0   51   10    0 3346   48   54    8]\n",
      " [   0    0  118   18    4   63  479   53    2]\n",
      " [   0    0   23    1    2   43   37 1987   18]\n",
      " [   0    0    0    1    0   11    2   47 1257]]\n"
     ]
    }
   ],
   "source": [
    "#2. Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_LR = confusion_matrix(y_val, y_pred_LR_val)\n",
    "print(cm_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "j7m42RV2hhxi",
    "outputId": "de6af0e6-d43b-4b5d-deed-a85e711ef8c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.99      0.78      0.87       460\n",
      "          2       0.89      0.97      0.93      4005\n",
      "          3       0.70      0.78      0.74      1947\n",
      "          4       0.82      0.41      0.55       710\n",
      "          5       0.97      0.97      0.97       665\n",
      "          6       0.94      0.95      0.95      3517\n",
      "          7       0.79      0.65      0.71       737\n",
      "          8       0.93      0.94      0.93      2111\n",
      "          9       0.98      0.95      0.97      1318\n",
      "\n",
      "avg / total       0.89      0.89      0.89     15470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "cr_LR = classification_report(y_val, y_pred_LR_val)\n",
    "print(cr_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rRJfbK77hl9Q"
   },
   "source": [
    "We can see from the aove that the precision and recall is about 0.89 and f1 score is also 0.89 which is encouraging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "liYLoIF_h0rh"
   },
   "source": [
    "#### **cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "xleXkj0GhkSr",
    "outputId": "544c9fb1-3a33-4d14-8166-2db3daa1be98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_accuracy:0.894285\n",
      "best_std:0.001880\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cv_LR = cross_val_score(estimator=classifier_LR, X=X_train,\n",
    "                            y=y_train.ravel(), scoring='accuracy', cv=5)\n",
    "print('best_accuracy:%4f' %cv_LR.mean())\n",
    "print('best_std:%4f' %cv_LR.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r0IQVTZ8iYVg"
   },
   "source": [
    "We can see that the Cross validation accuracy is also about 89% and it looks good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gazXpTSwiCf7"
   },
   "source": [
    "## **KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9w87WH9lh5JY"
   },
   "outputs": [],
   "source": [
    "#Using an elbow method to find the neighbours\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "error_rate = []\n",
    "\n",
    "for i in range(1, 20):\n",
    "  classifier_KNN = KNeighborsClassifier(n_neighbors = i)\n",
    "  classifier_KNN.fit(X_train, y_train)\n",
    "  y_pred_val = classifier_KNN.predict(X_val)\n",
    "  error_rate.append(np.mean(y_pred_val != y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "colab_type": "code",
    "id": "iPsk2i5XikxV",
    "outputId": "1e8dbff0-1479-4cf4-ff71-64a64d19b0df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Error_Rate')"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmoAAAGCCAYAAABQLe8NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNnDTfwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f83f0d79128>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualizing\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,20), error_rate)\n",
    "plt.title('Error_Rate vs K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error_Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WnSZB-TiRaw7"
   },
   "source": [
    "We can see that the K>15 the error is around 0.824 and 0.823. So we put K=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-pHx5BHJvqz"
   },
   "outputs": [],
   "source": [
    "#KNN model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier_KNN = KNeighborsClassifier(n_neighbors=15)\n",
    "classifier_KNN.fit(X_train, y_train)\n",
    "y_pred_KNN_val = classifier_KNN.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "zKrYRfAXIzl8",
    "outputId": "aecea54e-3cdc-4a80-a55b-0593959e4e3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.82\n"
     ]
    }
   ],
   "source": [
    "#training accuracy\n",
    "print(round(classifier_KNN.score(X_train, y_train)*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GLIK1cK8S1iW"
   },
   "source": [
    "We can see that the training accuracy is about 85.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kGQzqfFBJMNr"
   },
   "source": [
    "#### **Evaluation of KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DjYqfXycJK9R"
   },
   "outputs": [],
   "source": [
    "#1. Accuracy of KNN\n",
    "from sklearn.metrics import accuracy_score\n",
    "val_acc_KNN = accuracy_score(y_val, y_pred_KNN_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "byOGNMK5VFZL",
    "outputId": "549abea6-3178-4301-a849-128469bb7338"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.833354880413704\n"
     ]
    }
   ],
   "source": [
    "print(val_acc_KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lgXzyDxyVJF5"
   },
   "source": [
    "We can see that the validation accuracy is about 83.3%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "QmlmDUFcKg6f",
    "outputId": "221646cc-9c02-4809-c8df-28cb24e537f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 340,   53,    3,    1,    0,   17,    1,   16,   29],\n",
       "       [   1, 3637,  333,   15,    5,    3,    9,    1,    1],\n",
       "       [   0,  707, 1182,   33,    0,    5,   15,    4,    1],\n",
       "       [   0,  202,  274,  204,    6,   15,    6,    2,    1],\n",
       "       [   0,    7,    9,    0,  646,    1,    0,    1,    1],\n",
       "       [   7,   14,   25,   16,   10, 3263,   66,   69,   47],\n",
       "       [   2,   16,   60,    7,    5,   49,  516,   71,   11],\n",
       "       [   2,    7,   16,    0,    4,   86,   40, 1917,   39],\n",
       "       [   3,    2,    0,    2,    1,   38,   11,   74, 1187]])"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_KNN = confusion_matrix(y_val, y_pred_KNN_val)\n",
    "cm_KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "a7d73BFuVQ8W",
    "outputId": "56654514-b928-4729-9a23-ac41a268cf51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.96      0.74      0.83       460\n",
      "          2       0.78      0.91      0.84      4005\n",
      "          3       0.62      0.61      0.61      1947\n",
      "          4       0.73      0.29      0.41       710\n",
      "          5       0.95      0.97      0.96       665\n",
      "          6       0.94      0.93      0.93      3517\n",
      "          7       0.78      0.70      0.74       737\n",
      "          8       0.89      0.91      0.90      2111\n",
      "          9       0.90      0.90      0.90      1318\n",
      "\n",
      "avg / total       0.83      0.83      0.83     15470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "cr_KNN = classification_report(y_val, y_pred_KNN_val)\n",
    "print(cr_KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DjFI27YHV1zr"
   },
   "source": [
    "We can see that the precision and recall are equal and f1score is also 0.83"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rdf6FPb_WE95"
   },
   "source": [
    "#### **cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "7TcXGcjcVsi8",
    "outputId": "791d3cde-3481-4d2b-b79f-9579d15b35a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best accuracy:0.832075\n",
      "best_std:0.005026\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cv_KNN = cross_val_score(estimator=classifier_KNN, X=X_train, y=y_train, scoring='accuracy', cv=5)\n",
    "print('best accuracy:%4f' %cv_KNN.mean())\n",
    "print('best_std:%4f' %cv_KNN.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMAvTiN4ZW42"
   },
   "source": [
    "We can see that the cross validation is also give the accuracy of 83.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8_cKHXDlZnLN"
   },
   "source": [
    "## **Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d3uoa8fLW9sG"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier_NB = GaussianNB()\n",
    "classifier_NB.fit(X_train,y_train)\n",
    "y_pred_NB_val = classifier_NB.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "_EYrKQeSaNAv",
    "outputId": "05fb2fc3-4065-48af-b049-30042834a871"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.59\n"
     ]
    }
   ],
   "source": [
    "#training_accuracy\n",
    "print(round(classifier_NB.score(X_train, y_train)*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tKslNhtpaaTH"
   },
   "source": [
    "We can see that the training accuracy is about 80.6%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5xBCYGPma8Xg"
   },
   "source": [
    "#### **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "RLVo1ZREaU4j",
    "outputId": "8172684d-a662-4bbc-e9c2-82cfa511a433"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.803425985778927\n"
     ]
    }
   ],
   "source": [
    "#1. validation accuracy\n",
    "val_acc_NB = accuracy_score(y_val, y_pred_NB_val)\n",
    "print(val_acc_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4mImiSdRa249"
   },
   "source": [
    "The validation accuracy is about 80.3%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "fPNKhL-Wa1jZ",
    "outputId": "089a5eea-b277-4342-b239-e39cd95c6673"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 359   94    0    0    0    5    0    2    0]\n",
      " [   7 3687  271    0    0   27    0   13    0]\n",
      " [   0  969  783  132   34   24    0    5    0]\n",
      " [   0   46   22  566   66    7    2    1    0]\n",
      " [   0   22    7    6  626    4    0    0    0]\n",
      " [   0   99   28   27   23 3172   60   79   29]\n",
      " [   0  157    3    0    0   44  515   15    3]\n",
      " [   0   44    0    0    0  109  134 1475  349]\n",
      " [   0    4    1    0    0   29    1   37 1246]]\n"
     ]
    }
   ],
   "source": [
    "#2. Confusion matrix\n",
    "cm_NB = confusion_matrix(y_val, y_pred_NB_val)\n",
    "print(cm_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "IJpFaGpfbexL",
    "outputId": "6ee4b9e2-7368-4fdd-f61b-8a1b9797ec1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.98      0.78      0.87       460\n",
      "          2       0.72      0.92      0.81      4005\n",
      "          3       0.70      0.40      0.51      1947\n",
      "          4       0.77      0.80      0.79       710\n",
      "          5       0.84      0.94      0.89       665\n",
      "          6       0.93      0.90      0.91      3517\n",
      "          7       0.72      0.70      0.71       737\n",
      "          8       0.91      0.70      0.79      2111\n",
      "          9       0.77      0.95      0.85      1318\n",
      "\n",
      "avg / total       0.81      0.80      0.80     15470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "cr_NB = classification_report(y_val, y_pred_NB_val)\n",
    "print(cr_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TREsi0-vbs0X"
   },
   "source": [
    "The precision and accuracy here is almost similar and the f1 score is also around 0.80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0GS-0bTGb7Oe"
   },
   "source": [
    "#### **cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "8nlKkYfebreL",
    "outputId": "8141ad54-0197-4ebb-91b9-5c2b4b0cde34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best accuracy: 0.804429\n",
      "best std: 0.007360\n"
     ]
    }
   ],
   "source": [
    "cv_NB = cross_val_score(estimator=classifier_NB, X=X_train, y=y_train, cv=10, scoring='accuracy')\n",
    "print('best accuracy: %4f' %cv_NB.mean())\n",
    "print('best std: %4f' %cv_NB.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "73fWds8LcjAf"
   },
   "source": [
    "We can see that the cross validation also gives the 80% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JzROQFE9cpPY"
   },
   "source": [
    "## **Support Vector Machines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "skF0hgRLcgmc"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier_SVC = SVC()\n",
    "classifier_SVC.fit(X_train, y_train)\n",
    "y_pred_SVC_val = classifier_SVC.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "4-WvgLj_dMz_",
    "outputId": "2c53b05c-29f7-4413-d9de-212e7c749f50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.71\n"
     ]
    }
   ],
   "source": [
    "#training accuracy\n",
    "print(round(classifier_SVC.score(X_train, y_train)*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DYHYax-7egYA"
   },
   "source": [
    "We got the training accuracy of 96.71%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zPMo6FsOdXvR"
   },
   "source": [
    "#### **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "o5VuXfT-dWw3",
    "outputId": "a56fa91f-640d-41e7-feff-cb8c72a31651"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9458952811893988\n"
     ]
    }
   ],
   "source": [
    "#1. validation accuracy\n",
    "val_acc_SVC = accuracy_score(y_val, y_pred_SVC_val)\n",
    "print(val_acc_SVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xCcNhxzPWy6w"
   },
   "source": [
    "It gives the validation accuracy of 94.5% and it looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "ocIVFDJUgB-0",
    "outputId": "093d671a-5922-49e3-e1e3-8b703df7e4fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 415   17    0    0    0    4    0   22    2]\n",
      " [   2 3893  103    1    0    0    4    2    0]\n",
      " [   0   79 1834   24    0    2    4    4    0]\n",
      " [   0    0  204  484    6   12    3    1    0]\n",
      " [   0    0   12    3  647    3    0    0    0]\n",
      " [   1    0    4    8    0 3413   46   35   10]\n",
      " [   0    0    1    3    0   32  646   55    0]\n",
      " [   0    0    0    0    0   14   17 2062   18]\n",
      " [   0    1    0    0    0    6    4   68 1239]]\n"
     ]
    }
   ],
   "source": [
    "#2. confusion matrix\n",
    "cm_NB = confusion_matrix(y_val, y_pred_SVC_val)\n",
    "print(cm_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "Hn9Rq1fKlQsc",
    "outputId": "51dc6b61-c852-45bc-d71b-0c25fb97c499"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.99      0.90      0.95       460\n",
      "          2       0.98      0.97      0.97      4005\n",
      "          3       0.85      0.94      0.89      1947\n",
      "          4       0.93      0.68      0.79       710\n",
      "          5       0.99      0.97      0.98       665\n",
      "          6       0.98      0.97      0.97      3517\n",
      "          7       0.89      0.88      0.88       737\n",
      "          8       0.92      0.98      0.95      2111\n",
      "          9       0.98      0.94      0.96      1318\n",
      "\n",
      "avg / total       0.95      0.95      0.95     15470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "cr_NB = classification_report(y_val, y_pred_SVC_val)\n",
    "print(cr_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1wiRtUc6XCXj"
   },
   "source": [
    "The above one shows that the precision , recall and f1-score are all high as 95%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fda7qt0dmh7B"
   },
   "source": [
    "#### **cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "rJF_AX1qmkNw",
    "outputId": "3520505d-f590-4fe9-9836-a7ca82b37146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best accuracy: 0.944319\n",
      "best std: 0.002512\n"
     ]
    }
   ],
   "source": [
    "cv_SVC = cross_val_score(estimator=classifier_SVC, X=X_train, y=y_train, cv=5, scoring='accuracy')\n",
    "print('best accuracy: %4f' %cv_SVC.mean())\n",
    "print('best std: %4f' %cv_SVC.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BvQZadvIaQjX"
   },
   "source": [
    "We can also see that the cross validation gives the 94.4% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "frOteMyhmz5Z"
   },
   "source": [
    "## **Kernel SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gUE6bFxFm2NC"
   },
   "outputs": [],
   "source": [
    "#KernelSVM\n",
    "from sklearn.svm import SVC\n",
    "classifier_Ksvm = SVC(kernel='linear')\n",
    "classifier_Ksvm.fit(X_train, y_train)\n",
    "y_pred_Ksvm_val = classifier_Ksvm.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "H9TDYsDDnK0h",
    "outputId": "015d0c96-6781-45aa-889d-13da5faf96b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.38\n"
     ]
    }
   ],
   "source": [
    "#training accuracy\n",
    "print(round(classifier_Ksvm.score(X_train, y_train)*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ebX34dPdA4F"
   },
   "source": [
    "Kernel SVM gives the training accuracy of 99.38%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_4R_m0vpnZhr"
   },
   "source": [
    "#### **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "iFlHBk3XnLLr",
    "outputId": "96b4ffb2-206a-4d62-ed1f-0d0d8f60192a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9889463477698772\n"
     ]
    }
   ],
   "source": [
    "#1.validation accuracy\n",
    "val_acc_Ksvm = accuracy_score(y_val, y_pred_Ksvm_val)\n",
    "print(val_acc_Ksvm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eByoOVcJdg30"
   },
   "source": [
    "The validation accuracy gives the accuracy of 98.9%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "ZSeOAep2pxEC",
    "outputId": "b9040b45-8226-4875-f9ea-a018a75020ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 448   12    0    0    0    0    0    0    0]\n",
      " [   7 3979   19    0    0    0    0    0    0]\n",
      " [   0   22 1911   13    1    0    0    0    0]\n",
      " [   0    0   14  689    6    1    0    0    0]\n",
      " [   0    0    0    7  656    2    0    0    0]\n",
      " [   0    0    1    1    2 3499   14    0    0]\n",
      " [   0    0    0    0    0   11  714   12    0]\n",
      " [   0    0    0    0    0    2    7 2096    6]\n",
      " [   0    0    0    0    0    0    0   11 1307]]\n"
     ]
    }
   ],
   "source": [
    "#2.Confusion matrix\n",
    "cm_Ksvm = confusion_matrix(y_val, y_pred_Ksvm_val)\n",
    "print(cm_Ksvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "4lWZNLr-qGmQ",
    "outputId": "62e16b1b-90a9-423c-9469-e3e939daf432"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.98      0.97      0.98       460\n",
      "          2       0.99      0.99      0.99      4005\n",
      "          3       0.98      0.98      0.98      1947\n",
      "          4       0.97      0.97      0.97       710\n",
      "          5       0.99      0.99      0.99       665\n",
      "          6       1.00      0.99      1.00      3517\n",
      "          7       0.97      0.97      0.97       737\n",
      "          8       0.99      0.99      0.99      2111\n",
      "          9       1.00      0.99      0.99      1318\n",
      "\n",
      "avg / total       0.99      0.99      0.99     15470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "cr_Ksvm = classification_report(y_val, y_pred_Ksvm_val)\n",
    "print(cr_Ksvm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZseSheIudo3-"
   },
   "source": [
    "Wow.. The precision , Recall and f1-score is also as high as 99%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dh-yAkgmqTa8"
   },
   "source": [
    "#### **cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "p70pqd7CqWVv",
    "outputId": "57a6cd8d-1fef-4fe5-d53f-59f98289355c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best accuracy: 0.989248\n",
      "best std: 0.001293\n"
     ]
    }
   ],
   "source": [
    "cv_Ksvm = cross_val_score(estimator=classifier_Ksvm, X=X_train, y=y_train, cv=10, scoring='accuracy')\n",
    "print('best accuracy: %4f' %cv_Ksvm.mean())\n",
    "print('best std: %4f' %cv_Ksvm.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oow0GDvNgur-"
   },
   "source": [
    "The cross validation accuracy is also about 98.9%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nNtqmt6yqmay"
   },
   "source": [
    "## **Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wEcJyveIqwLn"
   },
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier_DT = DecisionTreeClassifier()\n",
    "classifier_DT.fit(X_train, y_train)\n",
    "y_pred_DT_val = classifier_DT.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "erBOJWa8qzbe",
    "outputId": "293af9b4-e375-457d-df42-2c19877ffe37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.38\n"
     ]
    }
   ],
   "source": [
    "#training accuracy\n",
    "print(round(classifier_Ksvm.score(X_train, y_train)*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7VQsY5EihBjF"
   },
   "source": [
    "The training accuracy is about 99.38%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iBU9t_TyrhD2"
   },
   "source": [
    "#### **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "2-eU8bZarkLw",
    "outputId": "e7d0a836-560a-4e4c-b3ca-a4a31b26ec5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999353587588882\n"
     ]
    }
   ],
   "source": [
    "#1.validation accuracy\n",
    "val_acc_DT = accuracy_score(y_val, y_pred_DT_val)\n",
    "print(val_acc_DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hGO5wwgChG-G"
   },
   "source": [
    "The validation accuracy is about 99.9%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "n9CCnn9crsN4",
    "outputId": "a86473ba-b369-421c-acb0-21bd854388ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 460    0    0    0    0    0    0    0    0]\n",
      " [   0 4005    0    0    0    0    0    0    0]\n",
      " [   0    0 1947    0    0    0    0    0    0]\n",
      " [   0    0    0  710    0    0    0    0    0]\n",
      " [   0    0    0    0  665    0    0    0    0]\n",
      " [   0    0    0    0    1 3516    0    0    0]\n",
      " [   0    0    0    0    0    0  737    0    0]\n",
      " [   0    0    0    0    0    0    0 2111    0]\n",
      " [   0    0    0    0    0    0    0    0 1318]]\n"
     ]
    }
   ],
   "source": [
    "#2.Confusion matrix\n",
    "cm_DT = confusion_matrix(y_val, y_pred_DT_val)\n",
    "print(cm_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "7VBXdCc-ry4S",
    "outputId": "5f1787b0-6e52-4673-913a-a62b58e1eb61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00       460\n",
      "          2       1.00      1.00      1.00      4005\n",
      "          3       1.00      1.00      1.00      1947\n",
      "          4       1.00      1.00      1.00       710\n",
      "          5       1.00      1.00      1.00       665\n",
      "          6       1.00      1.00      1.00      3517\n",
      "          7       1.00      1.00      1.00       737\n",
      "          8       1.00      1.00      1.00      2111\n",
      "          9       1.00      1.00      1.00      1318\n",
      "\n",
      "avg / total       1.00      1.00      1.00     15470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "cr_DT = classification_report(y_val, y_pred_DT_val)\n",
    "print(cr_DT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hI0AKbs9hQBU"
   },
   "source": [
    "The precision, recall and f1-score is above as high as 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SH9LTaaPsAI4"
   },
   "source": [
    "#### **cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "YYqfmJ7zsCyB",
    "outputId": "9b396e52-f17b-410d-ce48-9ee117c55f06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best accuracy: 0.999849\n",
      "best std: 0.000194\n"
     ]
    }
   ],
   "source": [
    "cv_DT = cross_val_score(estimator=classifier_DT, X=X_train, y=y_train, cv=10, scoring='accuracy')\n",
    "print('best accuracy: %4f' %cv_DT.mean())\n",
    "print('best std: %4f' %cv_DT.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f8s8NVAJhZqX"
   },
   "source": [
    "Wow... Even the cross validation gives the accuracy of 99.9%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wDB7RpQXsMaT"
   },
   "source": [
    "## **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a8cDtZvRsPLM"
   },
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier_RF = RandomForestClassifier()\n",
    "classifier_RF.fit(X_train, y_train)\n",
    "y_pred_RF_val = classifier_RF.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "R5qNejwgs-xj",
    "outputId": "1304a9a8-159e-42a1-d3ac-5850377efb73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.96\n"
     ]
    }
   ],
   "source": [
    "#training accuracy\n",
    "print(round(classifier_RF.score(X_train, y_train)*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WP_2g3HKhi5V"
   },
   "source": [
    "The training accuracy is about 99.96%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iB5wE1I-tEtm"
   },
   "source": [
    "#### **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "BCr3M1OStHUy",
    "outputId": "b23f052c-e1de-4a8f-c300-fbbed159708d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9746606334841629\n"
     ]
    }
   ],
   "source": [
    "#1.validation accuracy\n",
    "val_acc_RF = accuracy_score(y_val, y_pred_RF_val)\n",
    "print(val_acc_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VqT7YvQRhn9d"
   },
   "source": [
    "The validation accuracy is about 97.4%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "_hivz4ZStQZU",
    "outputId": "97efeac8-52f6-4a3a-bfc6-e0c1fe9360c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 431   12    0    0    0    9    1    5    2]\n",
      " [   3 3997    3    0    0    2    0    0    0]\n",
      " [   0   14 1921    7    0    2    2    1    0]\n",
      " [   1    3   28  656    4   11    5    1    1]\n",
      " [   0    0    0    3  660    2    0    0    0]\n",
      " [   1    2   10   11    0 3442   38   10    3]\n",
      " [   0    2   21   12    1   49  628   24    0]\n",
      " [   0    1    5    0    1   20    8 2074    2]\n",
      " [   1    0    2    0    0   13    4   29 1269]]\n"
     ]
    }
   ],
   "source": [
    "#2.Confusion matrix\n",
    "cm_RF = confusion_matrix(y_val, y_pred_RF_val)\n",
    "print(cm_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "74ErniZstVKr",
    "outputId": "5ee61ef9-20f6-4aea-c105-3b5c3b2b34e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.99      0.94      0.96       460\n",
      "          2       0.99      1.00      0.99      4005\n",
      "          3       0.97      0.99      0.98      1947\n",
      "          4       0.95      0.92      0.94       710\n",
      "          5       0.99      0.99      0.99       665\n",
      "          6       0.97      0.98      0.97      3517\n",
      "          7       0.92      0.85      0.88       737\n",
      "          8       0.97      0.98      0.97      2111\n",
      "          9       0.99      0.96      0.98      1318\n",
      "\n",
      "avg / total       0.97      0.97      0.97     15470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "cr_RF = classification_report(y_val, y_pred_RF_val)\n",
    "print(cr_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5YyquuNYhvg0"
   },
   "source": [
    "The precision, recall and f1-score gives the value of 97%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JxotVbszt_UO"
   },
   "source": [
    "#### **cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ihtwi2mMuCKJ",
    "outputId": "b79eb519-7a72-4139-ffdb-0e4e4f21b4a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best accuracy: 0.981189\n",
      "best std: 0.005553\n"
     ]
    }
   ],
   "source": [
    "cv_RF = cross_val_score(estimator=classifier_RF, X=X_train, y=y_train, cv=10, scoring='accuracy')\n",
    "print('best accuracy: %4f' %cv_RF.mean())\n",
    "print('best std: %4f' %cv_RF.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dQXPS46ch9SM"
   },
   "source": [
    "The cross validation gives the accuracy of 98.1%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6UxfEoGDuJPG"
   },
   "source": [
    "## **Ada Boost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "83TI9FSRuRzh"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "classifier_AB = AdaBoostClassifier()\n",
    "classifier_AB.fit(X_train, y_train)\n",
    "y_pred_AB_val = classifier_AB.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "92GN0LSruWVq",
    "outputId": "d7cf7ab5-bcd5-4feb-f149-25579007a22e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.68\n"
     ]
    }
   ],
   "source": [
    "#training accuracy\n",
    "print(round(classifier_AB.score(X_train, y_train)*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VH_wxQbQiG6I"
   },
   "source": [
    "Ada-Boost gives the training accuracy of 73.6%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vVALpJXcuik5"
   },
   "source": [
    "#### **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "v4fXo1Z0uWNH",
    "outputId": "dddbfde4-95cf-4868-cd9f-d872d7bb4443"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7376212023270847\n"
     ]
    }
   ],
   "source": [
    "#1.validation accuracy\n",
    "val_acc_AB = accuracy_score(y_val, y_pred_AB_val)\n",
    "print(val_acc_AB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GQCHPd7liOWN"
   },
   "source": [
    "The validation accuracy is 73.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "Q5id0uTJus5o",
    "outputId": "e5b79fef-00f6-431e-9d5c-a19033756130"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 460    0    0    0    0    0    0    0    0]\n",
      " [   0 4005    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0 1947    0    0    0]\n",
      " [   0    0    0    0    0  710    0    0    0]\n",
      " [   0    0    0    0    0  665    0    0    0]\n",
      " [   0    0    0    0    0 3517    0    0    0]\n",
      " [   0    0    0    0    0  737    0    0    0]\n",
      " [   0    0    0    0    0    0    0 2111    0]\n",
      " [   0    0    0    0    0    0    0    0 1318]]\n"
     ]
    }
   ],
   "source": [
    "#2.Confusion matrix\n",
    "cm_AB = confusion_matrix(y_val, y_pred_AB_val)\n",
    "print(cm_AB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "mHTZqY7auyg0",
    "outputId": "28820a64-6381-4994-bfe8-bb930a117b5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00       460\n",
      "          2       1.00      1.00      1.00      4005\n",
      "          3       0.00      0.00      0.00      1947\n",
      "          4       0.00      0.00      0.00       710\n",
      "          5       0.00      0.00      0.00       665\n",
      "          6       0.46      1.00      0.63      3517\n",
      "          7       0.00      0.00      0.00       737\n",
      "          8       1.00      1.00      1.00      2111\n",
      "          9       1.00      1.00      1.00      1318\n",
      "\n",
      "avg / total       0.62      0.74      0.65     15470\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "cr_AB = classification_report(y_val, y_pred_AB_val)\n",
    "print(cr_AB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AiwjY8GOiXJo"
   },
   "source": [
    "The precision is 62%, recall is 74% and the f1-score is 65% which is a poor performance by AdaBoost Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gRpyAQ6ru43W"
   },
   "source": [
    "#### **cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "KbQ1FguPu7sy",
    "outputId": "ead5ddfe-c359-40c3-b9f3-cb39ebb57e95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best accuracy: 0.704700\n",
      "best std: 0.070840\n"
     ]
    }
   ],
   "source": [
    "cv_AB = cross_val_score(estimator=classifier_AB, X=X_train, y=y_train, cv=10, scoring='accuracy')\n",
    "print('best accuracy: %4f' %cv_AB.mean())\n",
    "print('best std: %4f' %cv_AB.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7BD4aGQXiojR"
   },
   "source": [
    "Also the cross validation is not good for AdaBoost since its accuracy is 70.4%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxOqdY-nvGvN"
   },
   "source": [
    "## **Gradient Boosting Machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZFSCSwHRvMKc"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "classifier_GBM = GradientBoostingClassifier()\n",
    "classifier_GBM.fit(X_train, y_train)\n",
    "y_pred_GBM_val = classifier_GBM.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "UT4zx17uvjGD",
    "outputId": "ba9c3245-1989-41cb-f83d-96bf8acc1656"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "#training accuracy\n",
    "print(round(classifier_GBM.score(X_train, y_train)*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ju8HhHqSjkG0"
   },
   "source": [
    "Gradient Boosting gives the training accuracy of 100%. There is a high percentage that it could be overfiiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jShCBiy6vpfr"
   },
   "source": [
    "#### **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "wFyGFzO4vrgq",
    "outputId": "0d11dda7-1cb5-47b0-d297-94821062e96f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#1.validation accuracy\n",
    "val_acc_GBM = accuracy_score(y_val, y_pred_GBM_val)\n",
    "print(val_acc_GBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3iwUv7GZkA3z"
   },
   "source": [
    "Wow the validation accuracy is also 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "OxlDQEO5wjN3",
    "outputId": "30e9a7b8-1dba-48ed-bc42-f1e46ffdc83a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 460    0    0    0    0    0    0    0    0]\n",
      " [   0 4005    0    0    0    0    0    0    0]\n",
      " [   0    0 1947    0    0    0    0    0    0]\n",
      " [   0    0    0  710    0    0    0    0    0]\n",
      " [   0    0    0    0  665    0    0    0    0]\n",
      " [   0    0    0    0    0 3517    0    0    0]\n",
      " [   0    0    0    0    0    0  737    0    0]\n",
      " [   0    0    0    0    0    0    0 2111    0]\n",
      " [   0    0    0    0    0    0    0    0 1318]]\n"
     ]
    }
   ],
   "source": [
    "#2.Confusion matrix\n",
    "cm_GBM = confusion_matrix(y_val, y_pred_GBM_val)\n",
    "print(cm_GBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "MzWSPVZqwpJc",
    "outputId": "b8dac930-fdd5-4c64-af7e-88f63cd385e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00       460\n",
      "          2       1.00      1.00      1.00      4005\n",
      "          3       1.00      1.00      1.00      1947\n",
      "          4       1.00      1.00      1.00       710\n",
      "          5       1.00      1.00      1.00       665\n",
      "          6       1.00      1.00      1.00      3517\n",
      "          7       1.00      1.00      1.00       737\n",
      "          8       1.00      1.00      1.00      2111\n",
      "          9       1.00      1.00      1.00      1318\n",
      "\n",
      "avg / total       1.00      1.00      1.00     15470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "cr_GBM = classification_report(y_val, y_pred_GBM_val)\n",
    "print(cr_GBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gi1lhtJVkIqa"
   },
   "source": [
    "The precision, recall and f1-score is also as high as 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "umdLRftmwu2d"
   },
   "source": [
    "#### **cross validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "lj0M0vRawuUK",
    "outputId": "f40342f4-e4cd-4954-e4e8-0b626634e8df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best accuracy: 0.999871\n",
      "best std: 0.000198\n"
     ]
    }
   ],
   "source": [
    "cv_GBM = cross_val_score(estimator=classifier_GBM, X=X_train, y=y_train, cv=10, scoring='accuracy')\n",
    "print('best accuracy: %4f' %cv_GBM.mean())\n",
    "print('best std: %4f' %cv_GBM.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "isx5rWiBo3rp"
   },
   "source": [
    "Wow.. The validation accuracy is also as good as 99.9%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7vzRVXCwnCXd"
   },
   "source": [
    "## **XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6suAG9vJnGsD"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "classifier_xgb = XGBClassifier()\n",
    "classifier_xgb.fit(X_train, y_train)\n",
    "y_pred_xgb_val = classifier_xgb.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "id": "0QlbdewTncgc",
    "outputId": "bb3fb2f0-e33e-4a86-d247-14a97a2f9d50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#training accuracy\n",
    "print(round(classifier_xgb.score(X_train, y_train)*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UtGJenu4pvVX"
   },
   "source": [
    "Wow the training accuracy of XGBoost classifier is also 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7Hhsg5tnrCV"
   },
   "source": [
    "#### **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "bIalfkWUnuAp",
    "outputId": "060dee40-c3bb-4f7b-8b57-80130b6b72b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# 1.Validation accuracy\n",
    "val_acc_xgb = accuracy_score(y_val, y_pred_xgb_val)\n",
    "print(val_acc_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yUGEJWSep2FM"
   },
   "source": [
    "Validation accuracy is also 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "RDrkV5vKoLm8",
    "outputId": "84a3c0a7-8dae-45f2-d443-4a05ff7b5af9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 460    0    0    0    0    0    0    0    0]\n",
      " [   0 4005    0    0    0    0    0    0    0]\n",
      " [   0    0 1947    0    0    0    0    0    0]\n",
      " [   0    0    0  710    0    0    0    0    0]\n",
      " [   0    0    0    0  665    0    0    0    0]\n",
      " [   0    0    0    0    0 3517    0    0    0]\n",
      " [   0    0    0    0    0    0  737    0    0]\n",
      " [   0    0    0    0    0    0    0 2111    0]\n",
      " [   0    0    0    0    0    0    0    0 1318]]\n"
     ]
    }
   ],
   "source": [
    "# 2.Confusion matrix\n",
    "cm_xgb = confusion_matrix(y_val, y_pred_xgb_val)\n",
    "print(cm_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "h8qSSnm5olL_",
    "outputId": "ce766938-bdb1-4128-d369-8d86ad1ef29e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00       460\n",
      "          2       1.00      1.00      1.00      4005\n",
      "          3       1.00      1.00      1.00      1947\n",
      "          4       1.00      1.00      1.00       710\n",
      "          5       1.00      1.00      1.00       665\n",
      "          6       1.00      1.00      1.00      3517\n",
      "          7       1.00      1.00      1.00       737\n",
      "          8       1.00      1.00      1.00      2111\n",
      "          9       1.00      1.00      1.00      1318\n",
      "\n",
      "avg / total       1.00      1.00      1.00     15470\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "cr_xgb = classification_report(y_val, y_pred_xgb_val)\n",
    "print(cr_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9yXEE8P8p_hB"
   },
   "source": [
    "The precision, Recall and f1-score is also as high as 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aXTdZiOZqHun"
   },
   "source": [
    "We are going to tune the Suport Vector Machines with the linear Kernel which gives the good results before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pN81KbMGxNqu"
   },
   "source": [
    "## **Hyper Parameters Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yuKicA1oxQmB"
   },
   "outputs": [],
   "source": [
    "#finding the best parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [\n",
    "                {\n",
    "                    'C' : [1, 10, 100],\n",
    "                    'kernel' : ['linear'] \n",
    "                }\n",
    "             ]\n",
    "\n",
    "gridsearch = GridSearchCV(estimator=classifier_Ksvm, param_grid=parameters, scoring='accuracy', cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qm6-qAdOsa83"
   },
   "outputs": [],
   "source": [
    "grid_search = gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "FudQSsIX8Tq6",
    "outputId": "2b3845c5-1e8f-4d95-9c28-53e3ac00f1fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9914454404413032"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_accuracy = grid_search.best_score_\n",
    "best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "qPYgEzBduQbl",
    "outputId": "e336b0a9-dd86-4fd6-d412-0ba4ff13cab8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 100, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameters = grid_search.best_params_\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rPovSJ5HxrmU"
   },
   "outputs": [],
   "source": [
    "#Tuned model\n",
    "best_classifier = SVC(kernel='linear' , C= 100 )\n",
    "best_classifier.fit(X_train, y_train)\n",
    "y_pred_val_best = best_classifier.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "LtE2wJgJySHN",
    "outputId": "f226b2b7-8a01-4634-e957-b4ef01053758"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.92\n"
     ]
    }
   ],
   "source": [
    "#training accuracy\n",
    "print(round(best_classifier.score(X_train, y_train)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "_3P7GkIIytuo",
    "outputId": "a12f8957-a026-40c9-93be-f9de7048ecba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9914673561732386\n"
     ]
    }
   ],
   "source": [
    "#validation accuracy\n",
    "val_acc_best = accuracy_score(y_val, y_pred_val_best)\n",
    "print(val_acc_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R6mA0iuvf45b"
   },
   "source": [
    "We can see that the training and validation accuracy are giving almost similar results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0vyNz-ery37K"
   },
   "outputs": [],
   "source": [
    "#Prediction of test dataset\n",
    "y_pred = best_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KRYoCvMtgGte"
   },
   "source": [
    "## That's the end of the code!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "otto_classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
